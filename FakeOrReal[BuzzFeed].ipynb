{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first part, try to load jason file (not really useful, dont run it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BuzzFeed_Fake_91-Webpage.json\n",
      "BuzzFeed_Fake_69-Webpage.json\n",
      "BuzzFeed_Fake_83-Webpage.json\n",
      "BuzzFeed_Fake_29-Webpage.json\n",
      "BuzzFeed_Fake_84-Webpage.json\n",
      "BuzzFeed_Fake_18-Webpage.json\n",
      "BuzzFeed_Fake_58-Webpage.json\n",
      "BuzzFeed_Fake_85-Webpage.json\n",
      ".DS_Store\n",
      "BuzzFeed_Fake_28-Webpage.json\n",
      "BuzzFeed_Fake_82-Webpage.json\n",
      "BuzzFeed_Fake_68-Webpage.json\n",
      "BuzzFeed_Fake_90-Webpage.json\n",
      "BuzzFeed_Fake_59-Webpage.json\n",
      "BuzzFeed_Fake_19-Webpage.json\n",
      "BuzzFeed_Fake_13-Webpage.json\n",
      "BuzzFeed_Fake_1-Webpage.json\n",
      "BuzzFeed_Fake_46-Webpage.json\n",
      "BuzzFeed_Fake_14-Webpage.json\n",
      "BuzzFeed_Fake_41-Webpage.json\n",
      "BuzzFeed_Fake_6-Webpage.json\n",
      "BuzzFeed_Fake_54-Webpage.json\n",
      "BuzzFeed_Fake_53-Webpage.json\n",
      "BuzzFeed_Fake_30-Webpage.json\n",
      "BuzzFeed_Fake_65-Webpage.json\n",
      "BuzzFeed_Fake_37-Webpage.json\n",
      "BuzzFeed_Fake_62-Webpage.json\n",
      "BuzzFeed_Fake_88-Webpage.json\n",
      "BuzzFeed_Fake_22-Webpage.json\n",
      "BuzzFeed_Fake_77-Webpage.json\n",
      "BuzzFeed_Fake_25-Webpage.json\n",
      "BuzzFeed_Fake_70-Webpage.json\n",
      "BuzzFeed_Fake_52-Webpage.json\n",
      "BuzzFeed_Fake_55-Webpage.json\n",
      "BuzzFeed_Fake_7-Webpage.json\n",
      "BuzzFeed_Fake_40-Webpage.json\n",
      "BuzzFeed_Fake_15-Webpage.json\n",
      "BuzzFeed_Fake_47-Webpage.json\n",
      "BuzzFeed_Fake_12-Webpage.json\n",
      "BuzzFeed_Fake_71-Webpage.json\n",
      "BuzzFeed_Fake_24-Webpage.json\n",
      "BuzzFeed_Fake_76-Webpage.json\n",
      "BuzzFeed_Fake_23-Webpage.json\n",
      "BuzzFeed_Fake_89-Webpage.json\n",
      "BuzzFeed_Fake_63-Webpage.json\n",
      "BuzzFeed_Fake_36-Webpage.json\n",
      "BuzzFeed_Fake_64-Webpage.json\n",
      "BuzzFeed_Fake_31-Webpage.json\n",
      "BuzzFeed_Fake_57-Webpage.json\n",
      "BuzzFeed_Fake_50-Webpage.json\n",
      "BuzzFeed_Fake_10-Webpage.json\n",
      "BuzzFeed_Fake_45-Webpage.json\n",
      "BuzzFeed_Fake_2-Webpage.json\n",
      "BuzzFeed_Fake_17-Webpage.json\n",
      "BuzzFeed_Fake_5-Webpage.json\n",
      "BuzzFeed_Fake_42-Webpage.json\n",
      "BuzzFeed_Fake_21-Webpage.json\n",
      "BuzzFeed_Fake_74-Webpage.json\n",
      "BuzzFeed_Fake_26-Webpage.json\n",
      "BuzzFeed_Fake_73-Webpage.json\n",
      "BuzzFeed_Fake_33-Webpage.json\n",
      "BuzzFeed_Fake_66-Webpage.json\n",
      "BuzzFeed_Fake_34-Webpage.json\n",
      "BuzzFeed_Fake_61-Webpage.json\n",
      "BuzzFeed_Fake_43-Webpage.json\n",
      "BuzzFeed_Fake_4-Webpage.json\n",
      "BuzzFeed_Fake_16-Webpage.json\n",
      "BuzzFeed_Fake_3-Webpage.json\n",
      "BuzzFeed_Fake_44-Webpage.json\n",
      "BuzzFeed_Fake_11-Webpage.json\n",
      "BuzzFeed_Fake_51-Webpage.json\n",
      "BuzzFeed_Fake_56-Webpage.json\n",
      "BuzzFeed_Fake_60-Webpage.json\n",
      "BuzzFeed_Fake_35-Webpage.json\n",
      "BuzzFeed_Fake_67-Webpage.json\n",
      "BuzzFeed_Fake_32-Webpage.json\n",
      "BuzzFeed_Fake_72-Webpage.json\n",
      "BuzzFeed_Fake_27-Webpage.json\n",
      "BuzzFeed_Fake_75-Webpage.json\n",
      "BuzzFeed_Fake_20-Webpage.json\n",
      "BuzzFeed_Fake_80-Webpage.json\n",
      "BuzzFeed_Fake_78-Webpage.json\n",
      "BuzzFeed_Fake_87-Webpage.json\n",
      "BuzzFeed_Fake_38-Webpage.json\n",
      "BuzzFeed_Fake_9-Webpage.json\n",
      "BuzzFeed_Fake_49-Webpage.json\n",
      "BuzzFeed_Fake_39-Webpage.json\n",
      "BuzzFeed_Fake_86-Webpage.json\n",
      "BuzzFeed_Fake_79-Webpage.json\n",
      "BuzzFeed_Fake_81-Webpage.json\n",
      "BuzzFeed_Fake_48-Webpage.json\n",
      "BuzzFeed_Fake_8-Webpage.json\n"
     ]
    }
   ],
   "source": [
    "# list all files under folder \n",
    "import os\n",
    "directory = '/Users/moranwang/Desktop/SummerQuarterDS/Capstone/FakeNewsNet/Data/BuzzFeed/FakeNewsContent'\n",
    "for filename in os.listdir(directory):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#data=json.loads('BuzzFeed_Fake_1-Webpage.json')\n",
    "path = '/Users/moranwang/Desktop/SummerQuarterDS/Capstone/FakeNewsNet/Data/BuzzFeed/FakeNewsContent/BuzzFeed_Fake_88-Webpage.json'\n",
    "with open(path, 'r') as myfile:\n",
    "    if len(myfile.readlines()) != 0:\n",
    "        myfile.seek(0)\n",
    "        data = json.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_img</th>\n",
       "      <th>text</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_data</th>\n",
       "      <th>canonical_link</th>\n",
       "      <th>images</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>summary</th>\n",
       "      <th>movies</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://rightwingnews.com/wp-content/uploads/20...</td>\n",
       "      <td>Hillary’s TOP Donor Country Just Auctioned Off...</td>\n",
       "      <td>[Terresa Monroe-hamilton]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'googlebot': 'noimageindex', 'og': {'site_nam...</td>\n",
       "      <td>http://rightwingnews.com/hillary-clinton-2/hil...</td>\n",
       "      <td>[http://1.gravatar.com/avatar/d35b77ff6c390071...</td>\n",
       "      <td>Hillary’s TOP Donor Country Just Auctioned Off...</td>\n",
       "      <td>http://rightwingnews.com/hillary-clinton-2/hil...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>{'$date': 1474912025000}</td>\n",
       "      <td>http://rightwingnews.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             top_img  \\\n",
       "0  http://rightwingnews.com/wp-content/uploads/20...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Hillary’s TOP Donor Country Just Auctioned Off...   \n",
       "\n",
       "                     authors keywords  \\\n",
       "0  [Terresa Monroe-hamilton]       []   \n",
       "\n",
       "                                           meta_data  \\\n",
       "0  {'googlebot': 'noimageindex', 'og': {'site_nam...   \n",
       "\n",
       "                                      canonical_link  \\\n",
       "0  http://rightwingnews.com/hillary-clinton-2/hil...   \n",
       "\n",
       "                                              images  \\\n",
       "0  [http://1.gravatar.com/avatar/d35b77ff6c390071...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Hillary’s TOP Donor Country Just Auctioned Off...   \n",
       "\n",
       "                                                 url summary movies  \\\n",
       "0  http://rightwingnews.com/hillary-clinton-2/hil...             []   \n",
       "\n",
       "               publish_date                    source  \n",
       "0  {'$date': 1474912025000}  http://rightwingnews.com  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read all files under same folder to dataframe (Start to Run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is the code to read all json file under a folder together Fake[BuzzFeed] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "myList = []\n",
    "my_df  = pd.DataFrame()\n",
    "directory = '/Users/moranwang/Desktop/SummerQuarterDS/Capstone/FakeNewsNet/Data/BuzzFeed/FakeNewsContent'\n",
    "file_names = os.listdir(directory)\n",
    "cleaned_file_name = [f for f in file_names if f.endswith('.json')]\n",
    "#for i, filename in zip(range(1),cleaned_file_name):\n",
    "for filename in cleaned_file_name:   \n",
    "    with open(os.path.join(directory,filename),'r',encoding=\"utf8\") as myfile:\n",
    "        #print(filename)\n",
    "        data = json.load(myfile) \n",
    "        myList.append(data)\n",
    "    #with open(directory + '/' + filename,'r' ,encoding=\"utf8\", errors='ignore') as myfile:\n",
    "    #    if len(myfile.readlines()) != 0:\n",
    "    #        myfile.seek(0)\n",
    "    #        data = json.load(myfile)\n",
    "    #        print(data)\n",
    "    #        #data.append(json.load(myfile))\n",
    "    #       #my_df = my_df.append(pd.DataFrame.from_dict(data, orient='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "# myList is a list of dictionary \n",
    "print(len(myList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is the code to read all json file under a folder together Real[BuzzFeed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList_real = []\n",
    "my_df_real  = pd.DataFrame()\n",
    "directory_real = '/Users/moranwang/Desktop/SummerQuarterDS/Capstone/FakeNewsNet/Data/BuzzFeed/RealNewsContent'\n",
    "file_names_real = os.listdir(directory_real)\n",
    "cleaned_file_name_real = [f for f in file_names_real if f.endswith('.json')]\n",
    "#for i, filename in zip(range(1),cleaned_file_name):\n",
    "for filename in cleaned_file_name_real:   \n",
    "    with open(os.path.join(directory_real,filename),'r',encoding=\"utf8\") as myfile:\n",
    "        #print(filename)\n",
    "        data = json.load(myfile) \n",
    "        myList_real.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "print(len(myList_real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to load glove model, the model includes 400,000 words and put these words in a dictionary Model, the dictionary like --- \"a\", [a vector of number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# function load glove model\n",
    "file = \"/Users/moranwang/Desktop/SummerQuarterDS/glove/glove.6B.50d.txt\"\n",
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "      \n",
    "    with open(gloveFile, encoding=\"utf8\" ) as f:\n",
    "        content = f.readlines()\n",
    "    model = {}\n",
    "    for line in content:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        if (word == \"rahami\"):\n",
    "            print(\"rahami exist in the list in this case\") # rahamim exist\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "          \n",
    "model= loadGloveModel(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkword is a function to check if a word in the news exist in the model, if model dosent contains this word, then return true --- in the next step, if dosen't contain, then make this word to a 0 vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a word appears in the glove.6B.50d.txt or not , if not return true\n",
    "def checkword (search_word):\n",
    "    if search_word not in model:\n",
    "        return (True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(checkword(\"rahami\")) # rahami not exist in the glove dictionary(model), should return true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef checkword (search_word):\\n    file = \"/Users/moranwang/Desktop/SummerQuarterDS/glove/glove.6B.50d.txt\"\\n    news_file = open(file)\\n    each_line = news_file.readlines()\\n    for el in each_line:\\n        line_split = el.split()\\n        word = line_split[0]\\n        if(word != search_word):\\n            #print(\"it doesn\\'t exist\", search_word)\\n            #per_word_vector = np.empty(50)\\n            #per_word_vector.fill(0)\\n            return (True)  \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if a word appears in the glove.6B.50d.txt, if the word not appears in the txt file, then set the vector to 0\n",
    "'''\n",
    "def checkword (search_word):\n",
    "    file = \"/Users/moranwang/Desktop/SummerQuarterDS/glove/glove.6B.50d.txt\"\n",
    "    news_file = open(file)\n",
    "    each_line = news_file.readlines()\n",
    "    for el in each_line:\n",
    "        line_split = el.split()\n",
    "        word = line_split[0]\n",
    "        if(word != search_word):\n",
    "            #print(\"it doesn't exist\", search_word)\n",
    "            #per_word_vector = np.empty(50)\n",
    "            #per_word_vector.fill(0)\n",
    "            return (True)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(checkword(\"rahami\"))\n",
    "#print(model['hello'])\n",
    "#print(model['rahami'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "per_new_text= []\n",
    "news_text = []\n",
    "temp = []\n",
    "file = \"/Users/moranwang/Desktop/SummerQuarterDS/glove/glove.6B.50d.txt\"\n",
    "#print(len(myList))\n",
    "for i in range (0,len(myList)): # i from 0 to 90\n",
    "    #print(i)\n",
    "    new = myList[i]\n",
    "    news_content = new['text']\n",
    "    new_content_token = re.sub(r'[^\\w]', ' ', news_content).lower().split()\n",
    "    new_content_token = [ word for word in new_content_token if word not in stopwords.words('english')]\n",
    "#print(new_content_token)\n",
    "#    print(new_content_token[0])\n",
    "#    print(model[new_content_token[0]])\n",
    "#    print(model[new_content_token[1]])\n",
    "#    print(model[new_content_token[2]])\n",
    "    for word in new_content_token:\n",
    "        #print(word)\n",
    "        if (checkword(word) == True): # if a word not appears in the glove txt, then set the vector to 0 \n",
    "            per_word_vector = np.empty(50)\n",
    "            per_word_vector.fill(0) \n",
    "        else:\n",
    "            per_word_vector = model[word]\n",
    "        per_new_text.append(per_word_vector)\n",
    "    news_text.append(per_new_text)\n",
    "            \n",
    "        #print(per_word_vector)\n",
    "        #print(per_word_vector.shape)   \n",
    "        #print(per_word_vector)\n",
    "        #print(news_text)\n",
    "#print(len(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23021\n",
      "[ 0.95281   -0.20608    0.55618   -0.46323    0.73354    0.029137\n",
      " -0.19367   -0.090066  -0.22958   -0.19058   -0.34857   -1.0231\n",
      "  0.743     -0.5489     0.88484   -0.14051    0.0040139  0.58448\n",
      "  0.10767   -0.44657   -0.43205    0.9868     0.78288    0.51513\n",
      "  0.85788   -1.7713    -0.88259   -0.59728    0.084934  -0.48112\n",
      "  3.9678     0.8893    -0.27064   -0.44094   -0.26213    0.085597\n",
      "  0.022099  -0.58376    0.10908    0.77973   -0.95447    0.40482\n",
      "  0.8941     0.65251    0.39858    0.20884   -1.3281    -0.10882\n",
      " -0.22822   -0.46303  ]\n",
      "[ 0.23918  -0.096261  0.024736 -0.592     0.51256  -0.035554 -1.1253\n",
      "  0.48754  -0.12025  -0.37303   0.30812  -0.081929 -0.14237   0.11731\n",
      "  0.65941   0.28794  -0.66977   0.049251 -0.31647  -1.0925   -0.25429\n",
      "  0.39473   0.16178   0.10083   0.85331  -1.1566   -0.76104   0.95558\n",
      "  0.55682  -0.36111   1.4445    0.15637   0.66717  -1.0544   -0.019116\n",
      "  0.1055    0.12556   0.23093   0.41279   0.24118  -0.30911   0.38222\n",
      " -0.30777   0.5155    0.54437  -0.65327   0.18656   0.032427  0.10057\n",
      "  0.19921 ]\n"
     ]
    }
   ],
   "source": [
    "#print(len(news_text)) -- 91\n",
    "print((len(news_text[90]))) #- 23021 \n",
    "print(news_text[90][0])  # check \n",
    "print((news_text[90][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert BuzzFed real new to vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "per_new_text_real= []\n",
    "news_text_real = []\n",
    "#print(len(myList))\n",
    "for i in range (0,len(myList_real)): # i from 0 to 90\n",
    "    #print(i)\n",
    "    new_real = myList_real[i]\n",
    "    news_content_real = new_real['text']\n",
    "    new_content_token_real = re.sub(r'[^\\w]', ' ', news_content_real).lower().split()\n",
    "    new_content_token_real = [ word for word in new_content_token_real if word not in stopwords.words('english')]\n",
    "    for word in new_content_token_real:\n",
    "        if (checkword(word) == True): # if a word not appears in the glove txt, then set the vector to 0 \n",
    "            per_word_vector_real = np.empty(50)\n",
    "            per_word_vector_real.fill(0) \n",
    "        else:\n",
    "            per_word_vector_real = model[word]\n",
    "            per_new_text_real.append(per_word_vector_real)\n",
    "    news_text_real.append(per_new_text_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30481\n",
      "[ 1.4675e-01  1.1692e+00  6.9416e-01 -6.1429e-02 -1.3677e-01  4.2015e-01\n",
      " -7.1600e-01  1.9014e-02 -5.2896e-01 -8.3643e-01 -1.8561e+00 -1.8324e-01\n",
      "  5.7648e-02 -3.1188e-01  2.4997e-02  4.5878e-02 -9.8728e-02 -2.1451e-01\n",
      "  1.4298e-01 -8.0809e-03 -1.4569e-01  3.8326e-01  6.3811e-01 -4.6426e-01\n",
      "  1.0953e+00 -2.1500e+00 -1.8462e-01  1.7380e-01 -5.0607e-01  5.7719e-04\n",
      "  5.2828e-01  6.6850e-01 -8.9692e-01 -3.4346e-01 -1.5456e-01 -9.7313e-01\n",
      " -6.9441e-01  5.9201e-01 -1.2194e+00 -1.3469e+00 -2.5691e-01  3.4537e-01\n",
      " -4.3824e-01 -9.6233e-02  2.9882e-01 -2.9174e-01 -4.7201e-01 -3.2221e-01\n",
      "  7.9279e-02  5.9419e-01]\n",
      "[ 0.23158   0.69964   0.43878  -0.31633   0.18509   0.45519  -0.52914\n",
      "  0.13019  -0.6016   -0.53644  -1.8771   -0.32314  -0.13149   0.05427\n",
      "  0.24486  -0.037868 -0.34769  -0.60215   0.72329  -0.47918   0.15473\n",
      "  0.4589    0.37807  -1.0418    0.43743  -2.6996    0.2478    0.68145\n",
      " -0.23022  -0.024437  2.133     0.46905  -1.1284   -1.0115   -0.3608\n",
      " -0.89718  -0.47746   0.46726  -0.88369  -0.99028  -0.69334   0.55325\n",
      " -0.44621  -0.1735   -0.070923  0.04234  -0.68205   0.68464  -0.43253\n",
      "  0.75606 ]\n"
     ]
    }
   ],
   "source": [
    "print((len(news_text_real[90]))) #- 30481\n",
    "print(news_text_real[90][0])  # check \n",
    "print((news_text_real[90][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add fake news vecotr and real new vector to one list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "BuzzFeed_news = []\n",
    "BuzzFeed_news.extend(news_text)  # fake news \n",
    "BuzzFeed_news.extend(news_text_real) # real news\n",
    "BuzzFeed_news = np.array(BuzzFeed_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "23021\n"
     ]
    }
   ],
   "source": [
    "#[buzzfed[each_news[each_words]]]\n",
    "print(len(BuzzFeed_news))\n",
    "print(type(BuzzFeed_news)) # buzz feed is a list of list, the first 91 is fake new snd last last 91 is real news\n",
    "print(type(BuzzFeed_news[0])) # a list \n",
    "print(len(BuzzFeed_news[0])) # numpy array # 23021 words in the last news "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create response variable y fake is 1 and real is 0, y is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# create a list to hold the response variable fake and real (1 and 0), the first 91 is fake and last 91 is real\n",
    "fake_y = [1] * 91\n",
    "real_y = [0]*91\n",
    "fake_real = []\n",
    "fake_real.extend(fake_y)\n",
    "fake_real.extend(real_y)\n",
    "print(len(fake_real))\n",
    "\n",
    "# one hot encode the output variable\n",
    "BuzzFeed_Y = np.array (fake_real)\n",
    "print(type(BuzzFeed_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(BuzzFeed_news)))\n",
    "model.add(Dense(BuzzFeed_Y, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=batch_size, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some examples or some useless code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people', 'noticed', 'something', 'odd', 'hillary', 's', 'outfit', 'debate', 'last', 'night', 'there', 's', 'lot', 'could', 'discussed', 'last', 'night', 's', 'debate', 'like', 'unfair', 'debate', 'moderator', 'asking', 'questions', 'help', 'hillary', 'that', 's', 'main', 'concern', 'right', 'now', 'one', 'thing', 'baffled', 'many', 'hillary', 'didn', 't', 'appear', 'usually', 'stupid', 'coughing', 'self', 'ready', 'answers', 'detailed', 'facts', 'almost', 'bizarre', 'sputter', 'reach', 'glass', 'water', 'odd', 'everyone', 'grandma', 'talking', 'latest', 'question', 'swirling', 'around', 'social', 'media', 'pictures', 'appeared', 'show', 'hillary', 'kind', 'flesh-colored', 'device', 'embedded', 'inside', 'ear', 'is', 'earpiece', 'wearing', 'something', 'fashionable', 'old', 'ladies', 'twitter', 'may', 'figured', 'answer', 'do', 'see', 'it', 'certainly', 'wouldn', 't', 'first', 'time', 'hillary', 'use', 'device', 'help', 'think', 'could', 'trick', 'light', 'shadows', 'puffy', 'zipper', 'perhaps', 'hillary', 'actually', 'hiding', 'cough', 'prevention', 'machine', 'hideous', 'feminist', 'pantsuit', 'hillary', 'clinton', 's', 'campaign', 'fervently', 'denying', 'wearing', 'earpiece', '101', 'things', 'young', 'adults', 'know', 'sir', 'john', 'hawkins', 'john', 'hawkins', 's', 'book', '101', 'things', 'young', 'adults', 'know', 'filled', 'lessons', 'newly', 'minted', 'adults', 'need', 'order', 'get', 'life', 'gleaned', 'lifetime', 'trial', 'error', 'writing', 'down', 'hawkins', 'provides', 'advice', 'everyone', 'benefit', 'short', 'digestible', 'chapters', 'buy', 'fox', 'news', 'reported', 'clinton', 'spokesman', 'nick', 'merrill', 'said', 'seen', 'photo', 'merely', 'reflection', 'tv', 'lights', 'flash', 'aren', 't', 'sure', 'going', 'inside', 'hillary', 's', 'little', 'head', 'last', 'night', 'interesting', 'email', 'posted', 'woods', 'clinton', 'huma', 'abedin', 'wikileaks', 'archive', 'released', 'did', 'u', 'take', 'earpiece', 'need', 'get', 'it', 'woods', 'hidden', 'sense', 'humor', 'escaped', 'twitter', 'she', 'can', 't', 'even', 'lie', 'without', 'help', 'gaggle', 'liars', 'earpiece', 'woods', 'added', 'she', 'literally', 'can', 't', 'help', 'herself', 'she', 's', 'kind', 'sociopath', 'lie', 'even', 'truth', 'beneficial', 'course', 'don', 't', 'actual', 'proof', 'earpiece', 'could', 'hearing', 'aid', 'hmmm', 'one', 'talked', 'hillary', 'wearing', 'earpiece', 'past', 'fumbled', 'speeches', 'claim', 'it', 's', 'merely', 'reflection', 'studio', 'lights', 'we', 'll', 'let', 'judge']\n"
     ]
    }
   ],
   "source": [
    "# example of the content of the first news\n",
    "# from the list of dictionary , delete all \\n or \\nn for one new\n",
    "news_text = []\n",
    "new = myList[0]\n",
    "news_content = new['text']\n",
    "result = []\n",
    "#print(news_content) # sentence\n",
    "new_content_token= news_content.replace('\\n', ' ').lower().split()\n",
    "#print(new_content_token) # a list of words\n",
    "# remove stopwords word like I , am , my, the ...\n",
    "new_content_token = [ word for word in new_content_token if word not in stopwords.words('english')]\n",
    "#print(new_content_token)\n",
    "# remove wired charactor like ?, ..., \"', multiple X, 101, #, :, it's, we'll\n",
    "for word in new_content_token: \n",
    "    sc = \"….?’,“”'#!×:\"\n",
    "    for char in sc:\n",
    "        word = word.replace(char,\" \")  \n",
    "    word = word.split()\n",
    "    #print(word)\n",
    "    result.extend(word)\n",
    "print(result)\n",
    "for element in result:\n",
    "    #print(element)\n",
    "    per_word_vector = model[element]\n",
    "    \n",
    "    #print(model[element])\n",
    "    #print(type(model[element])) # type of per_word_vector is numpy.ndarray \n",
    "    \n",
    "    \n",
    "# convert each word in new_content_token to vector using word2vec or glove vector\n",
    "#print(model['hello'])\n",
    "#print(len(model['hello']))\n",
    "#for word in new_content_token:\n",
    "    #print(model[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model[element]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hillarys\n",
      "hillary\n"
     ]
    }
   ],
   "source": [
    "# example to delete ' and s\n",
    "a = 'hillary’s'\n",
    "b = \"’s\"\n",
    "for char in b:\n",
    "    a = a.replace(char,\"\")\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "people\n"
     ]
    }
   ],
   "source": [
    "#print(new_content_token)\n",
    "print(type(new_content_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['top_img', 'text', 'authors', 'keywords', 'meta_data', 'canonical_link', 'images', 'title', 'url', 'summary', 'movies', 'publish_date', 'source'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/moranwang/Desktop/SummerQuarterDS/Capstone/FakeNewsNet/Data/BuzzFeed/FakeNewsContent/BuzzFeed_Fake_80-Webpage.json\n"
     ]
    }
   ],
   "source": [
    "# concatenate string\n",
    "new = directory + '/'+ 'BuzzFeed_Fake_80-Webpage.json'\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example1 : convert text to vector Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is  Word2Vec(vocab=14, size=100, alpha=0.025)\n",
      "words is  ['this', 'is', 'the', 'first', 'sentence', 'for!', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
      "access vecotr for one word [ 4.14259126e-03  2.79425620e-03 -2.77865003e-03 -2.06513098e-03\n",
      " -3.35403159e-03  2.78954208e-03  3.93661810e-03  1.55871559e-03\n",
      " -1.44100119e-03 -4.32085153e-03  4.30899672e-03 -4.62976005e-03\n",
      " -3.79096786e-03  2.00172933e-03 -4.60096542e-03 -1.31963647e-03\n",
      "  2.37290107e-04  1.49146968e-03  2.41897837e-03  2.93111708e-03\n",
      "  3.15664662e-03 -2.23306101e-03 -5.75355079e-04  6.63872808e-04\n",
      "  4.13089339e-03  6.29035581e-04  1.40708033e-03 -6.88619097e-04\n",
      "  4.30145860e-03 -1.68555172e-03  2.99498509e-03  3.51108215e-03\n",
      " -1.11267739e-03  2.78331194e-04  3.57962656e-03  1.52858009e-03\n",
      "  1.35704526e-03  4.27913677e-04 -3.05608939e-03 -3.82844337e-05\n",
      "  3.61788506e-03 -3.90263507e-03  3.21124983e-03  3.75231705e-03\n",
      " -1.91865454e-03  3.26324417e-03 -2.85139750e-03  4.45809821e-03\n",
      " -3.87078477e-03 -2.92970566e-03  1.41498307e-03 -4.44768881e-03\n",
      " -3.77919595e-03 -3.34367529e-03  2.98122969e-03  1.60577567e-03\n",
      "  3.74832586e-03  2.37878994e-03  4.49375995e-03  2.26435461e-03\n",
      "  3.65829142e-03  4.24659904e-03  1.20491313e-03  2.75844592e-03\n",
      " -4.29223431e-03  2.13282625e-03 -1.41087407e-03  4.16299375e-03\n",
      "  4.09058155e-03  4.15608147e-03 -1.64551137e-03  1.15078839e-03\n",
      " -1.17498355e-04  4.58725216e-03 -2.38788105e-03 -2.73633376e-03\n",
      " -2.41419431e-04 -1.50865759e-03  1.57385634e-03 -4.32245340e-03\n",
      " -1.45382155e-03 -4.26712725e-03 -3.09640728e-03  4.69793472e-03\n",
      " -3.86413536e-03 -2.22670473e-03  7.12592911e-04  4.82412812e-04\n",
      "  4.75252187e-03  4.06654039e-03 -1.40219997e-03  1.67945094e-04\n",
      " -1.29531452e-03  2.44904472e-03 -5.84905138e-05 -3.72893550e-03\n",
      " -4.21523256e-03  3.28962272e-03  4.88315336e-03  3.00551183e-03]\n",
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1130fce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#word2vec example1/ using own word embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for!', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print('model is ',model)\n",
    "# summarize vocabulary,all words that appears in the sentences\n",
    "words = list(model.wv.vocab)\n",
    "print('words is ',words)\n",
    "# access vector for one word\n",
    "print('access vecotr for one word', model['sentence'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "# plot \n",
    "X = model[model.wv.vocab]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example2: golve / not run this code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n",
      "[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
      " -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
      "  0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
      "  0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
      "  0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
      " -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
      " -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
      "  0.67204 ]\n"
     ]
    }
   ],
   "source": [
    "file = \"/Users/moranwang/Desktop/SummerQuarterDS/glove/glove.6B.50d.txt\"\n",
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    \n",
    "     \n",
    "    with open(gloveFile, encoding=\"utf8\" ) as f:\n",
    "       content = f.readlines()\n",
    "    model = {}\n",
    "    for line in content:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "     \n",
    "     \n",
    "model= loadGloveModel(file)   \n",
    " \n",
    "print (model['hello'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of using  LSTM Recurrent Neural Networks in Python with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the Alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25}\n",
      "{0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}\n"
     ]
    }
   ],
   "source": [
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "print(char_to_int)\n",
    "print(int_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n",
      "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print(seq_in, '->', seq_out)\n",
    "print(dataX)\n",
    "print(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0]]\n",
      "\n",
      " [[ 1]]\n",
      "\n",
      " [[ 2]]\n",
      "\n",
      " [[ 3]]\n",
      "\n",
      " [[ 4]]\n",
      "\n",
      " [[ 5]]\n",
      "\n",
      " [[ 6]]\n",
      "\n",
      " [[ 7]]\n",
      "\n",
      " [[ 8]]\n",
      "\n",
      " [[ 9]]\n",
      "\n",
      " [[10]]\n",
      "\n",
      " [[11]]\n",
      "\n",
      " [[12]]\n",
      "\n",
      " [[13]]\n",
      "\n",
      " [[14]]\n",
      "\n",
      " [[15]]\n",
      "\n",
      " [[16]]\n",
      "\n",
      " [[17]]\n",
      "\n",
      " [[18]]\n",
      "\n",
      " [[19]]\n",
      "\n",
      " [[20]]\n",
      "\n",
      " [[21]]\n",
      "\n",
      " [[22]]\n",
      "\n",
      " [[23]]\n",
      "\n",
      " [[24]]]\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.        ]]\n",
      "\n",
      " [[0.00147929]]\n",
      "\n",
      " [[0.00295858]]\n",
      "\n",
      " [[0.00443787]]\n",
      "\n",
      " [[0.00591716]]\n",
      "\n",
      " [[0.00739645]]\n",
      "\n",
      " [[0.00887574]]\n",
      "\n",
      " [[0.01035503]]\n",
      "\n",
      " [[0.01183432]]\n",
      "\n",
      " [[0.01331361]]\n",
      "\n",
      " [[0.0147929 ]]\n",
      "\n",
      " [[0.01627219]]\n",
      "\n",
      " [[0.01775148]]\n",
      "\n",
      " [[0.01923077]]\n",
      "\n",
      " [[0.02071006]]\n",
      "\n",
      " [[0.02218935]]\n",
      "\n",
      " [[0.02366864]]\n",
      "\n",
      " [[0.02514793]]\n",
      "\n",
      " [[0.02662722]]\n",
      "\n",
      " [[0.02810651]]\n",
      "\n",
      " [[0.0295858 ]]\n",
      "\n",
      " [[0.03106509]]\n",
      "\n",
      " [[0.03254438]]\n",
      "\n",
      " [[0.03402367]]\n",
      "\n",
      " [[0.03550296]]]\n"
     ]
    }
   ],
   "source": [
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Models:\n",
    "1. Naive LSTM for Learning One-Char to One-Char Mapping (have problem)\n",
    "2. Naive LSTM for a Three-Char Feature Window to One-Char Mapping (have problem)\n",
    "3. Naive LSTM for a Three-Char Time Step Window to One-Char Mapping (have problem)\n",
    "4. LSTM State Within A Batch (good model) \n",
    "5. Stateful LSTM for a One-Char to One-Char Mapping\n",
    "6. LSTM with Variable-Length Input to One-Char Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will frame the problem as a random collection of one-letter input to one-letter output pairs. \n",
    "# As we will see this is a difficult framing of the problem for the LSTM to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Variable-Length Input to One-Char Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learns random subsequences of the alphabet and an effort to build a model that can be given arbitrary letters or subsequences of letters and predict the next letter in the alphabet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with Variable Length Input Sequences to One Character Output\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from theano.tensor.shared_randomstreams import RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQRST -> U\n",
      "W -> X\n",
      "O -> P\n",
      "OPQ -> R\n",
      "IJKLM -> N\n",
      "QRSTU -> V\n",
      "ABCD -> E\n",
      "X -> Y\n",
      "GHIJ -> K\n",
      "M -> N\n",
      "XY -> Z\n",
      "QRST -> U\n",
      "ABC -> D\n",
      "JKLMN -> O\n",
      "OP -> Q\n",
      "XY -> Z\n",
      "D -> E\n",
      "T -> U\n",
      "B -> C\n",
      "QRSTU -> V\n",
      "HIJ -> K\n",
      "JKLM -> N\n",
      "ABCDE -> F\n",
      "X -> Y\n",
      "V -> W\n",
      "DE -> F\n",
      "DEFG -> H\n",
      "BCDE -> F\n",
      "EFGH -> I\n",
      "BCDE -> F\n",
      "FG -> H\n",
      "RST -> U\n",
      "TUV -> W\n",
      "STUV -> W\n",
      "LMN -> O\n",
      "P -> Q\n",
      "MNOP -> Q\n",
      "JK -> L\n",
      "MNOP -> Q\n",
      "OPQRS -> T\n",
      "UVWXY -> Z\n",
      "PQRS -> T\n",
      "D -> E\n",
      "EFGH -> I\n",
      "IJK -> L\n",
      "WX -> Y\n",
      "STUV -> W\n",
      "MNOPQ -> R\n",
      "P -> Q\n",
      "WXY -> Z\n",
      "Epoch 1/500\n",
      " - 1s - loss: 3.2632 - acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.2132 - acc: 0.0800\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.1570 - acc: 0.0800\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.0627 - acc: 0.0800\n",
      "Epoch 5/500\n",
      " - 0s - loss: 2.9682 - acc: 0.0800\n",
      "Epoch 6/500\n",
      " - 0s - loss: 2.9126 - acc: 0.0800\n",
      "Epoch 7/500\n",
      " - 0s - loss: 2.8873 - acc: 0.0800\n",
      "Epoch 8/500\n",
      " - 0s - loss: 2.8627 - acc: 0.0800\n",
      "Epoch 9/500\n",
      " - 0s - loss: 2.8430 - acc: 0.0800\n",
      "Epoch 10/500\n",
      " - 0s - loss: 2.8245 - acc: 0.1000\n",
      "Epoch 11/500\n",
      " - 0s - loss: 2.7976 - acc: 0.0800\n",
      "Epoch 12/500\n",
      " - 0s - loss: 2.7660 - acc: 0.0800\n",
      "Epoch 13/500\n",
      " - 0s - loss: 2.7464 - acc: 0.1400\n",
      "Epoch 14/500\n",
      " - 0s - loss: 2.6989 - acc: 0.1200\n",
      "Epoch 15/500\n",
      " - 0s - loss: 2.6712 - acc: 0.1200\n",
      "Epoch 16/500\n",
      " - 0s - loss: 2.6330 - acc: 0.1200\n",
      "Epoch 17/500\n",
      " - 0s - loss: 2.6062 - acc: 0.1000\n",
      "Epoch 18/500\n",
      " - 0s - loss: 2.5885 - acc: 0.1000\n",
      "Epoch 19/500\n",
      " - 0s - loss: 2.5624 - acc: 0.1400\n",
      "Epoch 20/500\n",
      " - 0s - loss: 2.5396 - acc: 0.1000\n",
      "Epoch 21/500\n",
      " - 0s - loss: 2.5192 - acc: 0.1800\n",
      "Epoch 22/500\n",
      " - 0s - loss: 2.4988 - acc: 0.1400\n",
      "Epoch 23/500\n",
      " - 0s - loss: 2.4750 - acc: 0.1800\n",
      "Epoch 24/500\n",
      " - 0s - loss: 2.4718 - acc: 0.1600\n",
      "Epoch 25/500\n",
      " - 0s - loss: 2.4469 - acc: 0.2000\n",
      "Epoch 26/500\n",
      " - 0s - loss: 2.4282 - acc: 0.1600\n",
      "Epoch 27/500\n",
      " - 0s - loss: 2.4093 - acc: 0.1600\n",
      "Epoch 28/500\n",
      " - 0s - loss: 2.3948 - acc: 0.2000\n",
      "Epoch 29/500\n",
      " - 0s - loss: 2.3862 - acc: 0.2400\n",
      "Epoch 30/500\n",
      " - 0s - loss: 2.3819 - acc: 0.1600\n",
      "Epoch 31/500\n",
      " - 0s - loss: 2.3528 - acc: 0.2600\n",
      "Epoch 32/500\n",
      " - 0s - loss: 2.3322 - acc: 0.2200\n",
      "Epoch 33/500\n",
      " - 0s - loss: 2.3288 - acc: 0.2800\n",
      "Epoch 34/500\n",
      " - 0s - loss: 2.3024 - acc: 0.2600\n",
      "Epoch 35/500\n",
      " - 0s - loss: 2.2954 - acc: 0.2200\n",
      "Epoch 36/500\n",
      " - 0s - loss: 2.2734 - acc: 0.2600\n",
      "Epoch 37/500\n",
      " - 0s - loss: 2.2541 - acc: 0.3200\n",
      "Epoch 38/500\n",
      " - 0s - loss: 2.2529 - acc: 0.2600\n",
      "Epoch 39/500\n",
      " - 0s - loss: 2.2302 - acc: 0.2600\n",
      "Epoch 40/500\n",
      " - 0s - loss: 2.2030 - acc: 0.3200\n",
      "Epoch 41/500\n",
      " - 0s - loss: 2.1853 - acc: 0.3000\n",
      "Epoch 42/500\n",
      " - 0s - loss: 2.1742 - acc: 0.3000\n",
      "Epoch 43/500\n",
      " - 0s - loss: 2.1436 - acc: 0.3000\n",
      "Epoch 44/500\n",
      " - 0s - loss: 2.1301 - acc: 0.3200\n",
      "Epoch 45/500\n",
      " - 0s - loss: 2.1131 - acc: 0.3400\n",
      "Epoch 46/500\n",
      " - 0s - loss: 2.0899 - acc: 0.3200\n",
      "Epoch 47/500\n",
      " - 0s - loss: 2.0626 - acc: 0.3000\n",
      "Epoch 48/500\n",
      " - 0s - loss: 2.0677 - acc: 0.3200\n",
      "Epoch 49/500\n",
      " - 0s - loss: 2.0314 - acc: 0.3600\n",
      "Epoch 50/500\n",
      " - 0s - loss: 2.0060 - acc: 0.3600\n",
      "Epoch 51/500\n",
      " - 0s - loss: 1.9940 - acc: 0.3400\n",
      "Epoch 52/500\n",
      " - 0s - loss: 1.9728 - acc: 0.4000\n",
      "Epoch 53/500\n",
      " - 0s - loss: 1.9422 - acc: 0.3800\n",
      "Epoch 54/500\n",
      " - 0s - loss: 1.9312 - acc: 0.3400\n",
      "Epoch 55/500\n",
      " - 0s - loss: 1.9018 - acc: 0.3400\n",
      "Epoch 56/500\n",
      " - 0s - loss: 1.8969 - acc: 0.3600\n",
      "Epoch 57/500\n",
      " - 0s - loss: 1.8880 - acc: 0.4000\n",
      "Epoch 58/500\n",
      " - 0s - loss: 1.8706 - acc: 0.3800\n",
      "Epoch 59/500\n",
      " - 0s - loss: 1.8467 - acc: 0.4400\n",
      "Epoch 60/500\n",
      " - 0s - loss: 1.8322 - acc: 0.3800\n",
      "Epoch 61/500\n",
      " - 0s - loss: 1.8150 - acc: 0.4000\n",
      "Epoch 62/500\n",
      " - 0s - loss: 1.8015 - acc: 0.4200\n",
      "Epoch 63/500\n",
      " - 0s - loss: 1.7877 - acc: 0.3800\n",
      "Epoch 64/500\n",
      " - 0s - loss: 1.7658 - acc: 0.4600\n",
      "Epoch 65/500\n",
      " - 0s - loss: 1.7417 - acc: 0.4000\n",
      "Epoch 66/500\n",
      " - 0s - loss: 1.7384 - acc: 0.4200\n",
      "Epoch 67/500\n",
      " - 0s - loss: 1.7205 - acc: 0.4400\n",
      "Epoch 68/500\n",
      " - 0s - loss: 1.7236 - acc: 0.3800\n",
      "Epoch 69/500\n",
      " - 0s - loss: 1.7018 - acc: 0.4200\n",
      "Epoch 70/500\n",
      " - 0s - loss: 1.6867 - acc: 0.4400\n",
      "Epoch 71/500\n",
      " - 0s - loss: 1.6596 - acc: 0.5000\n",
      "Epoch 72/500\n",
      " - 0s - loss: 1.6686 - acc: 0.4200\n",
      "Epoch 73/500\n",
      " - 0s - loss: 1.6277 - acc: 0.4200\n",
      "Epoch 74/500\n",
      " - 0s - loss: 1.6335 - acc: 0.5200\n",
      "Epoch 75/500\n",
      " - 0s - loss: 1.6075 - acc: 0.4600\n",
      "Epoch 76/500\n",
      " - 0s - loss: 1.6074 - acc: 0.4000\n",
      "Epoch 77/500\n",
      " - 0s - loss: 1.6085 - acc: 0.5200\n",
      "Epoch 78/500\n",
      " - 0s - loss: 1.5829 - acc: 0.5000\n",
      "Epoch 79/500\n",
      " - 0s - loss: 1.5840 - acc: 0.4800\n",
      "Epoch 80/500\n",
      " - 0s - loss: 1.5659 - acc: 0.5000\n",
      "Epoch 81/500\n",
      " - 0s - loss: 1.5515 - acc: 0.4400\n",
      "Epoch 82/500\n",
      " - 0s - loss: 1.5469 - acc: 0.4200\n",
      "Epoch 83/500\n",
      " - 0s - loss: 1.5541 - acc: 0.4800\n",
      "Epoch 84/500\n",
      " - 0s - loss: 1.5053 - acc: 0.5000\n",
      "Epoch 85/500\n",
      " - 0s - loss: 1.5172 - acc: 0.5200\n",
      "Epoch 86/500\n",
      " - 0s - loss: 1.4922 - acc: 0.5400\n",
      "Epoch 87/500\n",
      " - 0s - loss: 1.4811 - acc: 0.5200\n",
      "Epoch 88/500\n",
      " - 0s - loss: 1.4711 - acc: 0.5000\n",
      "Epoch 89/500\n",
      " - 0s - loss: 1.4640 - acc: 0.5400\n",
      "Epoch 90/500\n",
      " - 0s - loss: 1.4489 - acc: 0.5800\n",
      "Epoch 91/500\n",
      " - 0s - loss: 1.4679 - acc: 0.5200\n",
      "Epoch 92/500\n",
      " - 0s - loss: 1.4339 - acc: 0.5400\n",
      "Epoch 93/500\n",
      " - 0s - loss: 1.4315 - acc: 0.4600\n",
      "Epoch 94/500\n",
      " - 0s - loss: 1.4151 - acc: 0.5200\n",
      "Epoch 95/500\n",
      " - 0s - loss: 1.4057 - acc: 0.5200\n",
      "Epoch 96/500\n",
      " - 0s - loss: 1.3991 - acc: 0.5600\n",
      "Epoch 97/500\n",
      " - 0s - loss: 1.3926 - acc: 0.5600\n",
      "Epoch 98/500\n",
      " - 0s - loss: 1.3787 - acc: 0.5200\n",
      "Epoch 99/500\n",
      " - 0s - loss: 1.3727 - acc: 0.5000\n",
      "Epoch 100/500\n",
      " - 0s - loss: 1.3780 - acc: 0.5000\n",
      "Epoch 101/500\n",
      " - 0s - loss: 1.3615 - acc: 0.5400\n",
      "Epoch 102/500\n",
      " - 0s - loss: 1.3369 - acc: 0.5200\n",
      "Epoch 103/500\n",
      " - 0s - loss: 1.3456 - acc: 0.5400\n",
      "Epoch 104/500\n",
      " - 0s - loss: 1.3219 - acc: 0.5600\n",
      "Epoch 105/500\n",
      " - 0s - loss: 1.3212 - acc: 0.5400\n",
      "Epoch 106/500\n",
      " - 0s - loss: 1.3103 - acc: 0.5600\n",
      "Epoch 107/500\n",
      " - 0s - loss: 1.3079 - acc: 0.5200\n",
      "Epoch 108/500\n",
      " - 0s - loss: 1.2997 - acc: 0.5600\n",
      "Epoch 109/500\n",
      " - 0s - loss: 1.3006 - acc: 0.5600\n",
      "Epoch 110/500\n",
      " - 0s - loss: 1.2820 - acc: 0.5600\n",
      "Epoch 111/500\n",
      " - 0s - loss: 1.2973 - acc: 0.6000\n",
      "Epoch 112/500\n",
      " - 0s - loss: 1.3037 - acc: 0.5400\n",
      "Epoch 113/500\n",
      " - 0s - loss: 1.2556 - acc: 0.5600\n",
      "Epoch 114/500\n",
      " - 0s - loss: 1.2581 - acc: 0.6000\n",
      "Epoch 115/500\n",
      " - 0s - loss: 1.2687 - acc: 0.6000\n",
      "Epoch 116/500\n",
      " - 0s - loss: 1.2422 - acc: 0.6000\n",
      "Epoch 117/500\n",
      " - 0s - loss: 1.2263 - acc: 0.6400\n",
      "Epoch 118/500\n",
      " - 0s - loss: 1.2241 - acc: 0.6200\n",
      "Epoch 119/500\n",
      " - 0s - loss: 1.2252 - acc: 0.6400\n",
      "Epoch 120/500\n",
      " - 0s - loss: 1.2083 - acc: 0.6400\n",
      "Epoch 121/500\n",
      " - 0s - loss: 1.2148 - acc: 0.5400\n",
      "Epoch 122/500\n",
      " - 0s - loss: 1.1803 - acc: 0.6200\n",
      "Epoch 123/500\n",
      " - 0s - loss: 1.2053 - acc: 0.5600\n",
      "Epoch 124/500\n",
      " - 0s - loss: 1.1843 - acc: 0.6000\n",
      "Epoch 125/500\n",
      " - 0s - loss: 1.1927 - acc: 0.6200\n",
      "Epoch 126/500\n",
      " - 0s - loss: 1.1839 - acc: 0.6200\n",
      "Epoch 127/500\n",
      " - 0s - loss: 1.1672 - acc: 0.6400\n",
      "Epoch 128/500\n",
      " - 0s - loss: 1.1558 - acc: 0.6400\n",
      "Epoch 129/500\n",
      " - 0s - loss: 1.1782 - acc: 0.6000\n",
      "Epoch 130/500\n",
      " - 0s - loss: 1.1739 - acc: 0.6200\n",
      "Epoch 131/500\n",
      " - 0s - loss: 1.1487 - acc: 0.6000\n",
      "Epoch 132/500\n",
      " - 0s - loss: 1.1420 - acc: 0.6000\n",
      "Epoch 133/500\n",
      " - 0s - loss: 1.1459 - acc: 0.6400\n",
      "Epoch 134/500\n",
      " - 0s - loss: 1.1350 - acc: 0.6400\n",
      "Epoch 135/500\n",
      " - 0s - loss: 1.1261 - acc: 0.6400\n",
      "Epoch 136/500\n",
      " - 0s - loss: 1.1234 - acc: 0.6200\n",
      "Epoch 137/500\n",
      " - 0s - loss: 1.1129 - acc: 0.6200\n",
      "Epoch 138/500\n",
      " - 0s - loss: 1.1211 - acc: 0.6400\n",
      "Epoch 139/500\n",
      " - 0s - loss: 1.1100 - acc: 0.6000\n",
      "Epoch 140/500\n",
      " - 0s - loss: 1.1017 - acc: 0.6400\n",
      "Epoch 141/500\n",
      " - 0s - loss: 1.0964 - acc: 0.6600\n",
      "Epoch 142/500\n",
      " - 0s - loss: 1.0906 - acc: 0.6600\n",
      "Epoch 143/500\n",
      " - 0s - loss: 1.0751 - acc: 0.6400\n",
      "Epoch 144/500\n",
      " - 0s - loss: 1.0762 - acc: 0.6800\n",
      "Epoch 145/500\n",
      " - 0s - loss: 1.0821 - acc: 0.6600\n",
      "Epoch 146/500\n",
      " - 0s - loss: 1.0739 - acc: 0.6600\n",
      "Epoch 147/500\n",
      " - 0s - loss: 1.0788 - acc: 0.7000\n",
      "Epoch 148/500\n",
      " - 0s - loss: 1.0884 - acc: 0.6400\n",
      "Epoch 149/500\n",
      " - 0s - loss: 1.0634 - acc: 0.6800\n",
      "Epoch 150/500\n",
      " - 0s - loss: 1.0501 - acc: 0.6400\n",
      "Epoch 151/500\n",
      " - 0s - loss: 1.0315 - acc: 0.6200\n",
      "Epoch 152/500\n",
      " - 0s - loss: 1.0417 - acc: 0.6800\n",
      "Epoch 153/500\n",
      " - 0s - loss: 1.0650 - acc: 0.5800\n",
      "Epoch 154/500\n",
      " - 0s - loss: 1.0472 - acc: 0.6600\n",
      "Epoch 155/500\n",
      " - 0s - loss: 1.0332 - acc: 0.6600\n",
      "Epoch 156/500\n",
      " - 0s - loss: 1.0169 - acc: 0.6600\n",
      "Epoch 157/500\n",
      " - 0s - loss: 1.0238 - acc: 0.6400\n",
      "Epoch 158/500\n",
      " - 0s - loss: 1.0248 - acc: 0.6600\n",
      "Epoch 159/500\n",
      " - 0s - loss: 1.0123 - acc: 0.7000\n",
      "Epoch 160/500\n",
      " - 0s - loss: 1.0170 - acc: 0.6600\n",
      "Epoch 161/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 1.0061 - acc: 0.7000\n",
      "Epoch 162/500\n",
      " - 0s - loss: 1.0111 - acc: 0.6200\n",
      "Epoch 163/500\n",
      " - 0s - loss: 1.0090 - acc: 0.6200\n",
      "Epoch 164/500\n",
      " - 0s - loss: 0.9907 - acc: 0.6800\n",
      "Epoch 165/500\n",
      " - 0s - loss: 0.9907 - acc: 0.6800\n",
      "Epoch 166/500\n",
      " - 0s - loss: 0.9993 - acc: 0.6600\n",
      "Epoch 167/500\n",
      " - 0s - loss: 0.9663 - acc: 0.6600\n",
      "Epoch 168/500\n",
      " - 0s - loss: 0.9744 - acc: 0.6600\n",
      "Epoch 169/500\n",
      " - 0s - loss: 0.9739 - acc: 0.6800\n",
      "Epoch 170/500\n",
      " - 0s - loss: 0.9613 - acc: 0.7200\n",
      "Epoch 171/500\n",
      " - 0s - loss: 0.9723 - acc: 0.6800\n",
      "Epoch 172/500\n",
      " - 0s - loss: 0.9678 - acc: 0.6800\n",
      "Epoch 173/500\n",
      " - 0s - loss: 0.9588 - acc: 0.6800\n",
      "Epoch 174/500\n",
      " - 0s - loss: 0.9615 - acc: 0.6200\n",
      "Epoch 175/500\n",
      " - 0s - loss: 0.9477 - acc: 0.6800\n",
      "Epoch 176/500\n",
      " - 0s - loss: 0.9478 - acc: 0.6800\n",
      "Epoch 177/500\n",
      " - 0s - loss: 0.9701 - acc: 0.6800\n",
      "Epoch 178/500\n",
      " - 0s - loss: 0.9578 - acc: 0.6600\n",
      "Epoch 179/500\n",
      " - 0s - loss: 0.9358 - acc: 0.7000\n",
      "Epoch 180/500\n",
      " - 0s - loss: 0.9453 - acc: 0.6600\n",
      "Epoch 181/500\n",
      " - 0s - loss: 0.9217 - acc: 0.6800\n",
      "Epoch 182/500\n",
      " - 0s - loss: 0.9325 - acc: 0.6800\n",
      "Epoch 183/500\n",
      " - 0s - loss: 0.9122 - acc: 0.6600\n",
      "Epoch 184/500\n",
      " - 0s - loss: 0.9078 - acc: 0.6600\n",
      "Epoch 185/500\n",
      " - 0s - loss: 0.9206 - acc: 0.6600\n",
      "Epoch 186/500\n",
      " - 0s - loss: 0.9165 - acc: 0.7000\n",
      "Epoch 187/500\n",
      " - 0s - loss: 0.8997 - acc: 0.7200\n",
      "Epoch 188/500\n",
      " - 0s - loss: 0.9019 - acc: 0.6600\n",
      "Epoch 189/500\n",
      " - 0s - loss: 0.8913 - acc: 0.7000\n",
      "Epoch 190/500\n",
      " - 0s - loss: 0.8867 - acc: 0.7000\n",
      "Epoch 191/500\n",
      " - 0s - loss: 0.8948 - acc: 0.7000\n",
      "Epoch 192/500\n",
      " - 0s - loss: 0.8821 - acc: 0.7200\n",
      "Epoch 193/500\n",
      " - 0s - loss: 0.8821 - acc: 0.6600\n",
      "Epoch 194/500\n",
      " - 0s - loss: 0.8876 - acc: 0.6400\n",
      "Epoch 195/500\n",
      " - 0s - loss: 0.8920 - acc: 0.6600\n",
      "Epoch 196/500\n",
      " - 0s - loss: 0.8613 - acc: 0.7800\n",
      "Epoch 197/500\n",
      " - 0s - loss: 0.8672 - acc: 0.6600\n",
      "Epoch 198/500\n",
      " - 0s - loss: 0.9103 - acc: 0.7000\n",
      "Epoch 199/500\n",
      " - 0s - loss: 0.8897 - acc: 0.6800\n",
      "Epoch 200/500\n",
      " - 0s - loss: 0.8487 - acc: 0.7400\n",
      "Epoch 201/500\n",
      " - 0s - loss: 0.8624 - acc: 0.7200\n",
      "Epoch 202/500\n",
      " - 0s - loss: 0.8542 - acc: 0.7000\n",
      "Epoch 203/500\n",
      " - 0s - loss: 0.8358 - acc: 0.7200\n",
      "Epoch 204/500\n",
      " - 0s - loss: 0.8667 - acc: 0.6600\n",
      "Epoch 205/500\n",
      " - 0s - loss: 0.8560 - acc: 0.7000\n",
      "Epoch 206/500\n",
      " - 0s - loss: 0.8471 - acc: 0.7200\n",
      "Epoch 207/500\n",
      " - 0s - loss: 0.8409 - acc: 0.6600\n",
      "Epoch 208/500\n",
      " - 0s - loss: 0.8537 - acc: 0.7200\n",
      "Epoch 209/500\n",
      " - 0s - loss: 0.8221 - acc: 0.7000\n",
      "Epoch 210/500\n",
      " - 0s - loss: 0.8232 - acc: 0.7600\n",
      "Epoch 211/500\n",
      " - 0s - loss: 0.8118 - acc: 0.7200\n",
      "Epoch 212/500\n",
      " - 0s - loss: 0.8365 - acc: 0.7000\n",
      "Epoch 213/500\n",
      " - 0s - loss: 0.8407 - acc: 0.7000\n",
      "Epoch 214/500\n",
      " - 0s - loss: 0.8224 - acc: 0.7200\n",
      "Epoch 215/500\n",
      " - 0s - loss: 0.8056 - acc: 0.7400\n",
      "Epoch 216/500\n",
      " - 0s - loss: 0.8116 - acc: 0.7000\n",
      "Epoch 217/500\n",
      " - 0s - loss: 0.8150 - acc: 0.7400\n",
      "Epoch 218/500\n",
      " - 0s - loss: 0.8048 - acc: 0.7200\n",
      "Epoch 219/500\n",
      " - 0s - loss: 0.7948 - acc: 0.7400\n",
      "Epoch 220/500\n",
      " - 0s - loss: 0.8232 - acc: 0.7200\n",
      "Epoch 221/500\n",
      " - 0s - loss: 0.7972 - acc: 0.7200\n",
      "Epoch 222/500\n",
      " - 0s - loss: 0.8116 - acc: 0.6800\n",
      "Epoch 223/500\n",
      " - 0s - loss: 0.7761 - acc: 0.7400\n",
      "Epoch 224/500\n",
      " - 0s - loss: 0.8043 - acc: 0.7400\n",
      "Epoch 225/500\n",
      " - 0s - loss: 0.8024 - acc: 0.7200\n",
      "Epoch 226/500\n",
      " - 0s - loss: 0.7828 - acc: 0.7400\n",
      "Epoch 227/500\n",
      " - 0s - loss: 0.7883 - acc: 0.6800\n",
      "Epoch 228/500\n",
      " - 0s - loss: 0.7883 - acc: 0.6800\n",
      "Epoch 229/500\n",
      " - 0s - loss: 0.7657 - acc: 0.7400\n",
      "Epoch 230/500\n",
      " - 0s - loss: 0.7698 - acc: 0.7200\n",
      "Epoch 231/500\n",
      " - 0s - loss: 0.7634 - acc: 0.8200\n",
      "Epoch 232/500\n",
      " - 0s - loss: 0.7593 - acc: 0.7800\n",
      "Epoch 233/500\n",
      " - 0s - loss: 0.7667 - acc: 0.7000\n",
      "Epoch 234/500\n",
      " - 0s - loss: 0.7952 - acc: 0.7400\n",
      "Epoch 235/500\n",
      " - 0s - loss: 0.7710 - acc: 0.7400\n",
      "Epoch 236/500\n",
      " - 0s - loss: 0.7646 - acc: 0.7400\n",
      "Epoch 237/500\n",
      " - 0s - loss: 0.7711 - acc: 0.7400\n",
      "Epoch 238/500\n",
      " - 0s - loss: 0.7581 - acc: 0.7800\n",
      "Epoch 239/500\n",
      " - 0s - loss: 0.7308 - acc: 0.7800\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.7410 - acc: 0.7000\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.7642 - acc: 0.7200\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.8327 - acc: 0.6600\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.7985 - acc: 0.7000\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.7569 - acc: 0.7600\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.7449 - acc: 0.8000\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.7372 - acc: 0.7400\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.7197 - acc: 0.7800\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.7556 - acc: 0.7800\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.7373 - acc: 0.8000\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.7227 - acc: 0.8000\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.7269 - acc: 0.7600\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.7174 - acc: 0.7800\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.7028 - acc: 0.8200\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.7088 - acc: 0.7600\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.7069 - acc: 0.7600\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.6939 - acc: 0.8000\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.7093 - acc: 0.7600\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.7278 - acc: 0.7200\n",
      "Epoch 259/500\n",
      " - 0s - loss: 0.7519 - acc: 0.6800\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.7098 - acc: 0.7600\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.7067 - acc: 0.7600\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.6985 - acc: 0.7600\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.6966 - acc: 0.7600\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.7001 - acc: 0.8200\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.6939 - acc: 0.7800\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.6809 - acc: 0.7800\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.6846 - acc: 0.8000\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.6877 - acc: 0.7800\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.6867 - acc: 0.8000\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.6988 - acc: 0.7800\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.6856 - acc: 0.7800\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.6767 - acc: 0.7600\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.6707 - acc: 0.8000\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.6888 - acc: 0.7400\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.6883 - acc: 0.7600\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.6727 - acc: 0.7600\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.7432 - acc: 0.7200\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.7562 - acc: 0.7400\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.6871 - acc: 0.7400\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.6576 - acc: 0.7600\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.6495 - acc: 0.8000\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.6524 - acc: 0.7800\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.6571 - acc: 0.8000\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.6659 - acc: 0.7600\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.6782 - acc: 0.7400\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.6543 - acc: 0.8000\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.6799 - acc: 0.7600\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.6496 - acc: 0.8000\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.6445 - acc: 0.7800\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.6313 - acc: 0.8000\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.6281 - acc: 0.8200\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.6496 - acc: 0.7800\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.6263 - acc: 0.8000\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.6357 - acc: 0.7800\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.6610 - acc: 0.7400\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.6370 - acc: 0.8000\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.6646 - acc: 0.7600\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.6393 - acc: 0.8000\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.6311 - acc: 0.8200\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.6371 - acc: 0.7800\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.6366 - acc: 0.8000\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.6043 - acc: 0.8200\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.6600 - acc: 0.7600\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.6212 - acc: 0.8200\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.6092 - acc: 0.8200\n",
      "Epoch 306/500\n",
      " - 0s - loss: 0.6669 - acc: 0.7800\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.6066 - acc: 0.7800\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.6340 - acc: 0.7800\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.6078 - acc: 0.8200\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.6011 - acc: 0.8000\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.6131 - acc: 0.8200\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.6060 - acc: 0.8000\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.6119 - acc: 0.8200\n",
      "Epoch 314/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-6e78e3a82c09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;31m# summarize performance of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "num_inputs = 50\n",
    "max_len = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(num_inputs):\n",
    "    start = numpy.random.randint(len(alphabet)-2)\n",
    "    end = numpy.random.randint(start, min(start+max_len,len(alphabet)-1))\n",
    "    sequence_in = alphabet[start:end+1]\n",
    "    sequence_out = alphabet[end + 1]\n",
    "    dataX.append([char_to_int[char] for char in sequence_in])\n",
    "    dataY.append(char_to_int[sequence_out])\n",
    "    print (sequence_in, '->', sequence_out)\n",
    "# convert list of lists to array and pad sequences if needed\n",
    "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(X, (X.shape[0], max_len, 1))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], 1)))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=batch_size, verbose=2)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for i in range(20):\n",
    "    pattern_index = numpy.random.randint(len(dataX))\n",
    "    pattern = dataX[pattern_index]\n",
    "    x = pad_sequences([pattern], maxlen=max_len, dtype='float32')\n",
    "    x = numpy.reshape(x, (1, max_len, 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print (seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
