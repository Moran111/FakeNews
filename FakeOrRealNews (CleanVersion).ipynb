{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read all json files from different folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read real\n",
    "myList = []\n",
    "directory = '/Users/moranwang/Desktop/SummerQuarterDS/Capstone/FakeNewsNet/Data/BuzzFeed/FakeNewsContent'\n",
    "file_names = os.listdir(directory)\n",
    "cleaned_file_name = [f for f in file_names if f.endswith('.json')]\n",
    "#for i, filename in zip(range(1),cleaned_file_name):\n",
    "for filename in cleaned_file_name:   \n",
    "    with open(os.path.join(directory,filename),'r',encoding=\"utf8\") as myfile:\n",
    "        #print(filename)\n",
    "        data = json.load(myfile) \n",
    "        myList.append(data)\n",
    "        \n",
    "# read fake\n",
    "myList_real = []\n",
    "my_df_real  = pd.DataFrame()\n",
    "directory_real = '/Users/moranwang/Desktop/SummerQuarterDS/Capstone/FakeNewsNet/Data/BuzzFeed/RealNewsContent'\n",
    "file_names_real = os.listdir(directory_real)\n",
    "cleaned_file_name_real = [f for f in file_names_real if f.endswith('.json')]\n",
    "#for i, filename in zip(range(1),cleaned_file_name):\n",
    "for filename in cleaned_file_name_real:   \n",
    "    with open(os.path.join(directory_real,filename),'r',encoding=\"utf8\") as myfile:\n",
    "        #print(filename)\n",
    "        data = json.load(myfile) \n",
    "        myList_real.append(data)\n",
    "        \n",
    "# read fake news\n",
    "myList_fake = []\n",
    "my_df  = pd.DataFrame()\n",
    "directory_fake = '/Users/moranwang/Desktop/SummerQuarterDS/Capstone/FakeNewsNet/Data/PolitiFact/FakeNewsContent'\n",
    "file_names_fake = os.listdir(directory_fake)\n",
    "cleaned_file_name_fake = [f for f in file_names_fake if f.endswith('.json')]\n",
    "#for i, filename in zip(range(1),cleaned_file_name):\n",
    "for filename in cleaned_file_name_fake:   \n",
    "    with open(os.path.join(directory_fake,filename),'r',encoding=\"utf8\") as myfile:\n",
    "        #print(filename)\n",
    "        data = json.load(myfile) \n",
    "        myList_fake.append(data)\n",
    "        \n",
    "# read good news\n",
    "myList_real_p = []\n",
    "directory_real_p = '/Users/moranwang/Desktop/SummerQuarterDS/Capstone/FakeNewsNet/Data/PolitiFact/RealNewsContent'\n",
    "file_names_real_p = os.listdir(directory_real_p)\n",
    "cleaned_file_name_real_p = [f for f in file_names_real_p if f.endswith('.json')]\n",
    "#for i, filename in zip(range(1),cleaned_file_name):\n",
    "for filename in cleaned_file_name_real_p:   \n",
    "    with open(os.path.join(directory_real_p,filename),'r',encoding=\"utf8\") as myfile:\n",
    "        #print(filename)\n",
    "        data = json.load(myfile) \n",
    "        myList_real_p.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append all file together (fake and real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = myList + myList_fake + myList_real + myList_real_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    }
   ],
   "source": [
    "print(len(news_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract news content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# news_content_all has already delete the stopwards.words\n",
    "import re \n",
    "news_content_all = []\n",
    "for i in range (0,len(news_list)): # i from 0 to 90\n",
    "    per_new_text= []\n",
    "    news_dict = news_list[i]\n",
    "    news_content = news_dict['text']\n",
    "    new_content_token = re.sub(r'[^\\w]', ' ', news_content).lower().split()\n",
    "    new_content_token = [ word for word in new_content_token if word not in stopwords.words('english')]\n",
    "    news_content_all.append(new_content_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n",
      "232\n",
      "160\n",
      "the type of news_content_all is  <class 'list'>\n",
      "the type fo news content all [0] is <class 'list'>\n",
      "people\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(news_content_all))\n",
    "print(len(news_content_all[0]))\n",
    "print(len(news_content_all[90]))\n",
    "print(\"the type of news_content_all is \", type(news_content_all))\n",
    "print(\"the type fo news content all [0] is\", type(news_content_all[0]))\n",
    "print(news_content_all[0][0])\n",
    "# this news_content_all is a list of list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of list to a list of string, do we can use in the Tokenizer part\n",
    "news_content_string = [' '.join(x) for x in news_content_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "people noticed something odd hillary outfit debate last night lot could discussed last night debate like unfair debate moderator asking questions help hillary main concern right one thing baffled many hillary appear usually stupid coughing self ready answers detailed facts almost bizarre sputter reach glass water odd everyone grandma talking latest question swirling around social media pictures appeared show hillary kind flesh colored device embedded inside ear earpiece wearing something fashionable old ladies twitter may figured answer see certainly first time hillary use device help think could trick light shadows puffy zipper perhaps hillary actually hiding cough prevention machine hideous feminist pantsuit hillary clinton campaign fervently denying wearing earpiece 101 things young adults know sir john hawkins john hawkins book 101 things young adults know filled lessons newly minted adults need order get life gleaned lifetime trial error writing hawkins provides advice everyone benefit short digestible chapters buy fox news reported clinton spokesman nick merrill said seen photo merely reflection tv lights flash sure going inside hillary little head last night interesting email posted woods clinton huma abedin wikileaks archive released u take earpiece need get woods hidden sense humor escaped twitter even lie without help gaggle liars earpiece woods added literally help kind sociopath lie even truth beneficial course actual proof earpiece could hearing aid hmmm one talked hillary wearing earpiece past fumbled speeches claim merely reflection studio lights let judge\n"
     ]
    }
   ],
   "source": [
    "print(type(news_content_string))\n",
    "print(news_content_string[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to create news_content all, a list of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# news_content_all doesn't delete the stopwards words\\nimport re \\nnews_content_all = []\\nfor i in range (0,len(news_list)): # i from 0 to 90\\n    per_new_text= []\\n    news_dict = news_list[i]\\n    news_content = news_dict['text']\\n    new_content_token = re.sub(r'[^\\\\w]', ' ', news_content).lower()\\n    news_content_all.append(new_content_token)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# news_content_all doesn't delete the stopwards words\n",
    "import re \n",
    "news_content_all = []\n",
    "for i in range (0,len(news_list)): # i from 0 to 90\n",
    "    per_new_text= []\n",
    "    news_dict = news_list[i]\n",
    "    news_content = news_dict['text']\n",
    "    new_content_token = re.sub(r'[^\\w]', ' ', news_content).lower()\n",
    "    news_content_all.append(new_content_token)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(news_content_all[0])\\nprint(type(news_content_all[0]))\\nprint(type(news_content_all))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(news_content_all[0])\n",
    "print(type(news_content_all[0]))\n",
    "print(type(news_content_all))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count the word frequency in the entire news and save all words we want to keep , delete words that we think is not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_content_string is a list of string, that are used for embedding \n",
    "# news_content_all is a list of list \n",
    "# make nested list to flat list and then count frequency \n",
    "from collections import Counter\n",
    "import itertools\n",
    "flatList = list(itertools.chain.from_iterable(news_content_all))\n",
    "counts = Counter(flatList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n",
      "15605\n"
     ]
    }
   ],
   "source": [
    "print(type(counts))\n",
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n3798 word frequent is greater than 5. \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for key, cnts in list(counts.items()):   \n",
    "    if cnts > 5:\n",
    "        count = count + 1\n",
    "print(count) \n",
    "\n",
    "'''\n",
    "3798 word frequent is greater than 5. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create response variable y fake is 1 and real is 0, y is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# create a list to hold the response variable fake and real (1 and 0), the first 91 is fake and last 91 is real\n",
    "fake_y = [1] * (91 + 120)\n",
    "real_y = [0]* (91+120)\n",
    "fake_real_label = []\n",
    "fake_real_label.extend(fake_y)\n",
    "fake_real_label.extend(real_y)\n",
    "print(len(fake_real_label))\n",
    "print(type(fake_real_label))\n",
    "\n",
    "# one hot encode the output variable\n",
    "news_Y = np.array (fake_real_label)\n",
    "print(type(news_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs = train_X\n",
    "docs = news_content_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'people noticed something odd hillary outfit debate last night lot could discussed last night debate like unfair debate moderator asking questions help hillary main concern right one thing baffled many hillary appear usually stupid coughing self ready answers detailed facts almost bizarre sputter reach glass water odd everyone grandma talking latest question swirling around social media pictures appeared show hillary kind flesh colored device embedded inside ear earpiece wearing something fashionable old ladies twitter may figured answer see certainly first time hillary use device help think could trick light shadows puffy zipper perhaps hillary actually hiding cough prevention machine hideous feminist pantsuit hillary clinton campaign fervently denying wearing earpiece 101 things young adults know sir john hawkins john hawkins book 101 things young adults know filled lessons newly minted adults need order get life gleaned lifetime trial error writing hawkins provides advice everyone benefit short digestible chapters buy fox news reported clinton spokesman nick merrill said seen photo merely reflection tv lights flash sure going inside hillary little head last night interesting email posted woods clinton huma abedin wikileaks archive released u take earpiece need get woods hidden sense humor escaped twitter even lie without help gaggle liars earpiece woods added literally help kind sociopath lie even truth beneficial course actual proof earpiece could hearing aid hmmm one talked hillary wearing earpiece past fumbled speeches claim merely reflection studio lights let judge'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 把词转换为编号，词的编号根据词频设定，频率越大，编号越小\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_embbeding  = t.texts_to_sequences(docs)\n",
    "\n",
    "'''\n",
    "# 把词转换为编号，词的编号根据词频设定，频率越大，编号越小\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "232\n",
      "<class 'list'>\n",
      "[9, 46, 799, 800, 1956, 24, 332, 801, 294, 336, 6536, 2469, 9074, 801, 294, 284, 608, 1217, 1440, 9, 319, 6537, 89, 1218, 1296, 3803, 802, 6538, 1530, 117, 9075, 19, 742, 15, 1296, 608, 89, 2098, 258, 226, 163, 1219, 1619, 1724, 1006, 85, 3321, 89, 2098, 1837, 34, 3322, 132, 6539, 2272, 55, 801, 1441, 414, 2703, 3804, 4374, 1725, 2703, 514, 608, 589, 2, 258, 1297, 12, 5253, 6540, 2, 415, 2470, 608, 860, 138, 132, 4375, 63, 89, 1366, 415, 1006, 1619, 1724, 85, 3321, 1367, 2471, 30, 47, 6541, 5254, 1726, 3805, 609, 235, 6542, 416, 3806, 354, 61, 142, 169, 15, 826, 71, 210, 71, 210, 440, 354, 61, 142, 169, 15, 644, 764, 740, 898, 169, 74, 115, 20, 82, 858, 622, 566, 567, 672, 210, 741, 645, 141, 441, 247, 899, 900, 568, 608, 63, 24, 2704, 4376, 9076, 6543, 9077, 938, 320, 2705, 569, 699, 2472, 1099, 1440, 24, 164, 1442, 5255, 318, 6544, 6545, 9078, 590, 221, 623, 6542, 608, 452, 202, 28, 93, 453, 117, 742, 117, 1724, 249, 515, 2473, 5256, 180, 47, 167, 801, 802, 126, 1007, 6534, 13, 147, 646, 2273, 2099, 1619, 1008, 1530, 180, 47, 44, 4377, 187, 417, 148, 801, 144, 9079, 647, 13, 1726, 194, 9080, 801, 194, 1368, 3807, 1957, 18]\n"
     ]
    }
   ],
   "source": [
    "print(type(news_embbeding))\n",
    "print(len(news_embbeding[0]))\n",
    "print(type(news_embbeding[0]))\n",
    "print(news_embbeding[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 426\n",
    "padded_docs = pad_sequences(news_embbeding, maxlen=max_length) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npad_sequences is a function that can make different length of news become same length, if the length of a news exceed \\nthe maxlen, then this function will make the words that exceed max length to 0 and if the length of a news is less than\\nthe maxlen, then this function will make the different length of words between current length and max length to 0. \\n\\nSo, how to set the max length? how to determine what element should be left or delete? \\n\\nbefore doing pad_sequences, we decide to count frequency of words in all news then rank them. Save the words with \\nhighest and useful words. (except I , am , and, my)\\n\\nSaving the words that we want to keep to a new trainX and redo the new embedding. \\n\\n\\nBy the way, the embedding part is translate each word to a unqiue integer. \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pad_sequences is a function that can make different length of news become same length, if the length of a news exceed \n",
    "the maxlen, then this function will make the words that exceed max length to 0 and if the length of a news is less than\n",
    "the maxlen, then this function will make the different length of words between current length and max length to 0. \n",
    "\n",
    "So, how to set the max length? how to determine what element should be left or delete? \n",
    "\n",
    "before doing pad_sequences, we decide to count frequency of words in all news then rank them. Save the words with \n",
    "highest and useful words. (except I , am , and, my)\n",
    "\n",
    "Saving the words that we want to keep to a new trainX and redo the new embedding. \n",
    "\n",
    "\n",
    "By the way, the embedding part is translate each word to a unqiue integer. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795\n"
     ]
    }
   ],
   "source": [
    "# check how many words appears more than 5 times, \n",
    "count = 0\n",
    "for key, cnts in list(counts.items()):   \n",
    "    if cnts > 30:\n",
    "        count = count + 1\n",
    "print(count) \n",
    "\n",
    "# greater than 20, 1213 words\n",
    "# greater than 30, 795 words\n",
    "# greater than 50, 426 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncannot sort it, because after sort the order of words will change \\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cannot sort it, because after sort the order of words will change \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 3, 3, 5, 5, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 13, 14, 15, 15, 16, 16, 16, 17, 18, 20, 20, 22, 22, 22, 23, 23, 26, 31, 34, 35, 37, 39, 39, 39, 44, 52, 58, 61, 61, 66, 71, 71, 74, 74, 82, 84, 86, 86, 86, 105, 105, 106, 106, 115, 117, 126, 127, 127, 127, 127, 141, 141, 142, 142, 143, 146, 158, 166, 169, 169, 169, 177, 201, 209, 210, 210, 210, 216, 220, 234, 235, 241, 247, 248, 264, 264, 284, 317, 318, 330, 331, 335, 354, 354, 384, 396, 405, 440, 441, 451, 462, 463, 464, 464, 484, 495, 513, 534, 535, 551, 565, 566, 567, 568, 607, 622, 644, 645, 672, 697, 698, 724, 740, 741, 764, 796, 796, 797, 798, 826, 827, 857, 858, 859, 897, 898, 899, 900, 975, 1005, 1047, 1098, 1157, 1158, 1214, 1214, 1214, 1215, 1216, 1294, 1295, 1365, 1436, 1437, 1437, 1438, 1439, 1721, 1722, 1723, 1834, 1835, 1836, 1836, 1953, 1954, 1955, 2269, 2270, 2271, 2467, 2468, 2468, 2701, 2702, 2970, 2970, 2971, 2972, 2972, 2972, 3317, 3317, 3317, 3317, 3317, 3317, 3318, 3319, 3320, 3799, 3800, 3801, 3802, 4369, 4370, 4371, 4372, 4373, 5245, 5246, 5247, 5248, 5249, 5249, 5250, 5251, 5252, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 9067, 9068, 9069, 9070, 9071, 9072, 9073]\n"
     ]
    }
   ],
   "source": [
    "# pad_sequences process based on rank\n",
    "# Example to sort the first element\n",
    "e1 = news_embbeding[0]\n",
    "print(sorted(e1,key = int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(x) for x in news_embbeding]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([332.,  56.,  18.,  10.,   1.,   2.,   0.,   0.,   1.,   2.]),\n",
       " array([   0.,  323.,  646.,  969., 1292., 1615., 1938., 2261., 2584.,\n",
       "        2907., 3230.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fe0a518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171\n"
     ]
    }
   ],
   "source": [
    "length = sorted(length)\n",
    "length\n",
    "\n",
    "index = int(len(length)* 0.97) # 1171 cover 97% of words\n",
    "max_length = length[index]\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "padded_docs = pad_sequences(news_embbeding, maxlen=max_length, padding = 'post') \n",
    "\n",
    "\n",
    "\n",
    "# choose save the words which frequent greater than 50?\n",
    "# padding: String, 'pre' or 'post': pad either before or after each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   7 2467  105 ...    0    0    0]\n",
      " [   9   46  799 ...    0    0    0]\n",
      " [2474 2973  552 ...    0    0    0]\n",
      " ...\n",
      " [2724 2490 1009 ...    0    0    0]\n",
      " [2996 6588 1056 ...    0    0    0]\n",
      " [   8    3  239 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_docs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use glove to do word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "embeddings_index = dict()\n",
    "file = \"/Users/moranwang/Desktop/SummerQuarterDS/glove/glove.6B.50d.txt\"\n",
    "f = open(file)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Next, we need to create a matrix of one embedding for each word in the training dataset. \n",
    "We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight \n",
    "vector from the loaded GloVe embedding.\n",
    "The result is a matrix of weights only for words we will see during training.\n",
    "'''\n",
    "vocab_size = len(t.word_index) + 1\n",
    "from numpy import zeros\n",
    "embedding_matrix = zeros((vocab_size, 50))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15606\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(len(t.word_index)) #  how many words in the entire words\n",
    "print(type(t.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(t.word_index)\n",
    "\n",
    "value = [v for key,v in t.word_index.items()]\n",
    "value.count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using training data to train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "validation_size = 0.2\n",
    "seed = 3\n",
    "# write your code  to complete following line\n",
    "train_X, test_X = train_test_split(padded_docs,  test_size=validation_size, random_state=seed)\n",
    "train_y, test_y = train_test_split(news_Y,  test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1171\n",
      "1171\n",
      "337\n",
      "[0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0]\n",
      "[[  977  1241     1 ...     0     0     0]\n",
      " [10479  3183    81 ...     0     0     0]\n",
      " [  710   188   851 ...     0     0     0]\n",
      " ...\n",
      " [    4    10     1 ...     0     0     0]\n",
      " [  113   467  3823 ...     0     0     0]\n",
      " [  404  1038  5598 ...  1693   404  1038]]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_X[0]))\n",
    "print(len(train_X[1]))\n",
    "print(len(train_X))\n",
    "print(train_y[0:20])\n",
    "print(test_X[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# add embedding layers\n",
    "model.add(Embedding(vocab_size, 50,weights=[embedding_matrix], input_length=max_length)) # length of dictionary, dim output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.layers[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(40, input_shape =(None,50)))  # what is 20 means?  128 , 256, 512\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'val_accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 337 samples, validate on 85 samples\n",
      "Epoch 1/50\n",
      "337/337 [==============================] - 15s 44ms/step - loss: 0.6902 - acc: 0.5074 - val_loss: 0.7022 - val_acc: 0.4471\n",
      "Epoch 2/50\n",
      "337/337 [==============================] - 14s 41ms/step - loss: 0.6795 - acc: 0.5134 - val_loss: 0.7196 - val_acc: 0.4471\n",
      "Epoch 3/50\n",
      "337/337 [==============================] - 14s 41ms/step - loss: 0.6756 - acc: 0.5134 - val_loss: 0.7471 - val_acc: 0.4471\n",
      "Epoch 4/50\n",
      "337/337 [==============================] - 14s 41ms/step - loss: 0.6738 - acc: 0.4955 - val_loss: 0.7575 - val_acc: 0.5529\n",
      "Epoch 5/50\n",
      "337/337 [==============================] - 14s 41ms/step - loss: 0.6734 - acc: 0.5104 - val_loss: 0.7730 - val_acc: 0.5529\n",
      "Epoch 6/50\n",
      "337/337 [==============================] - 14s 41ms/step - loss: 0.6732 - acc: 0.5163 - val_loss: 0.7819 - val_acc: 0.5529\n",
      "Epoch 7/50\n",
      "337/337 [==============================] - 14s 41ms/step - loss: 0.6730 - acc: 0.5015 - val_loss: 0.7923 - val_acc: 0.4471\n",
      "Epoch 8/50\n",
      "337/337 [==============================] - 14s 41ms/step - loss: 0.6733 - acc: 0.5134 - val_loss: 0.8010 - val_acc: 0.4471\n",
      "Epoch 9/50\n",
      "337/337 [==============================] - 14s 42ms/step - loss: 0.6727 - acc: 0.5134 - val_loss: 0.8066 - val_acc: 0.5412\n",
      "Epoch 10/50\n",
      "160/337 [=============>................] - ETA: 7s - loss: 0.6673 - acc: 0.4750"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-00dc92474ea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(train_X, train_y, batch_size=20, validation_data= (test_X, test_y), epochs=50)\n",
    "# evaluate the model\n",
    "#loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
