{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'fake.csv'\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# unqiue types \n",
    "u_type = df['type'].unique()\n",
    "print(len(u_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs            11492\n",
      "bias            443\n",
      "conspiracy      430\n",
      "hate            246\n",
      "satire          146\n",
      "state           121\n",
      "junksci         102\n",
      "fake             19\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count in each type \n",
    "count_type = df['type'].value_counts()\n",
    "print(count_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12631e940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1270beeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw the count result\n",
    "count_type.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# which website has fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_df = df[df['type'] != 'bs']\n",
    "fake_news_website = fake_news_df['site_url'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>ord_in_thread</th>\n",
       "      <th>author</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>crawled</th>\n",
       "      <th>site_url</th>\n",
       "      <th>country</th>\n",
       "      <th>domain_rank</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>spam_score</th>\n",
       "      <th>main_img_url</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>participants_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>shares</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6a175f46bcd24d39b3e962ad0f29936721db70db</td>\n",
       "      <td>0</td>\n",
       "      <td>Barracuda Brigade</td>\n",
       "      <td>2016-10-26T21:41:00.000+03:00</td>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-10-27T01:49:27.168+03:00</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>US</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2bdc29d12605ef9cf3f09f9875040a7113be5d5b</td>\n",
       "      <td>0</td>\n",
       "      <td>reasoning with facts</td>\n",
       "      <td>2016-10-29T08:47:11.259+03:00</td>\n",
       "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
       "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-10-29T08:47:11.259+03:00</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>US</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c70e149fdd53de5e61c29281100b9de0ed268bc3</td>\n",
       "      <td>0</td>\n",
       "      <td>Barracuda Brigade</td>\n",
       "      <td>2016-10-31T01:41:49.479+02:00</td>\n",
       "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
       "      <td>Red State : \\nFox News Sunday reported this mo...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-10-31T01:41:49.479+02:00</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>US</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7cf7c15731ac2a116dd7f629bd57ea468ed70284</td>\n",
       "      <td>0</td>\n",
       "      <td>Fed Up</td>\n",
       "      <td>2016-11-01T05:22:00.000+02:00</td>\n",
       "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
       "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-11-01T15:46:26.304+02:00</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>US</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
       "      <td>0.068</td>\n",
       "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0206b54719c7e241ffe0ad4315b808290dbe6c0f</td>\n",
       "      <td>0</td>\n",
       "      <td>Fed Up</td>\n",
       "      <td>2016-11-01T21:56:00.000+02:00</td>\n",
       "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
       "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-11-01T23:59:42.266+02:00</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>US</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
       "      <td>0.865</td>\n",
       "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uuid  ord_in_thread  \\\n",
       "0  6a175f46bcd24d39b3e962ad0f29936721db70db              0   \n",
       "1  2bdc29d12605ef9cf3f09f9875040a7113be5d5b              0   \n",
       "2  c70e149fdd53de5e61c29281100b9de0ed268bc3              0   \n",
       "3  7cf7c15731ac2a116dd7f629bd57ea468ed70284              0   \n",
       "4  0206b54719c7e241ffe0ad4315b808290dbe6c0f              0   \n",
       "\n",
       "                 author                      published  \\\n",
       "0     Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n",
       "1  reasoning with facts  2016-10-29T08:47:11.259+03:00   \n",
       "2     Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n",
       "3                Fed Up  2016-11-01T05:22:00.000+02:00   \n",
       "4                Fed Up  2016-11-01T21:56:00.000+02:00   \n",
       "\n",
       "                                               title  \\\n",
       "0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
       "1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
       "2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
       "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
       "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
       "\n",
       "                                                text language  \\\n",
       "0  Print They should pay all the back all the mon...  english   \n",
       "1  Why Did Attorney General Loretta Lynch Plead T...  english   \n",
       "2  Red State : \\nFox News Sunday reported this mo...  english   \n",
       "3  Email Kayla Mueller was a prisoner and torture...  english   \n",
       "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  english   \n",
       "\n",
       "                         crawled             site_url country  domain_rank  \\\n",
       "0  2016-10-27T01:49:27.168+03:00  100percentfedup.com      US      25689.0   \n",
       "1  2016-10-29T08:47:11.259+03:00  100percentfedup.com      US      25689.0   \n",
       "2  2016-10-31T01:41:49.479+02:00  100percentfedup.com      US      25689.0   \n",
       "3  2016-11-01T15:46:26.304+02:00  100percentfedup.com      US      25689.0   \n",
       "4  2016-11-01T23:59:42.266+02:00  100percentfedup.com      US      25689.0   \n",
       "\n",
       "                                        thread_title  spam_score  \\\n",
       "0  Muslims BUSTED: They Stole Millions In Gov’t B...       0.000   \n",
       "1  Re: Why Did Attorney General Loretta Lynch Ple...       0.000   \n",
       "2  BREAKING: Weiner Cooperating With FBI On Hilla...       0.000   \n",
       "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...       0.068   \n",
       "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...       0.865   \n",
       "\n",
       "                                        main_img_url  replies_count  \\\n",
       "0  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
       "1  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
       "2  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
       "3  http://100percentfedup.com/wp-content/uploads/...              0   \n",
       "4  http://100percentfedup.com/wp-content/uploads/...              0   \n",
       "\n",
       "   participants_count  likes  comments  shares  type  \n",
       "0                   1      0         0       0  bias  \n",
       "1                   1      0         0       0  bias  \n",
       "2                   1      0         0       0  bias  \n",
       "3                   0      0         0       0  bias  \n",
       "4                   0      0         0       0  bias  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_website = fake_news_website.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(type(fake_news_website))\n",
    "print(len(fake_news_website))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fake news website and title of these fake news\n",
    "fake_news_title = fake_news_df[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1507\n"
     ]
    }
   ],
   "source": [
    "fake_news_title.head()\n",
    "print(len(fake_news_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add news content into a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_content_from_csv = fake_news_title['text'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print They should pay all the back all the money plus interest. The entire family and everyone who came in with them need to be deported asap. Why did it take two years to bust them? \n",
      "Here we go again …another group stealing from the government and taxpayers! A group of Somalis stole over four million in government benefits over just 10 months! \n",
      "We’ve reported on numerous cases like this one where the Muslim refugees/immigrants commit fraud by scamming our system…It’s way out of control! More Related\n"
     ]
    }
   ],
   "source": [
    "print(fake_news_content_from_csv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_content_from_csv_str = []\n",
    "for i in range(0,len(fake_news_content_from_csv)):\n",
    "    #print(type(fake_news_content_from_csv[i]))\n",
    "    if(isinstance(fake_news_content_from_csv[i], str)):\n",
    "        fake_news_content_from_csv_str.append(fake_news_content_from_csv[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fake_news_content_from_csv[1111]\n",
    "new_content_token_csv = re.sub(r'[^\\w]', ' ', a).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['breitbart', 'october', '26', '2016', 'a', 'syrian', 'christian', 'mother', 'shared', 'her', 'story', 'of', 'escaping', 'the', 'islamic', 'state', 'terrorist', 'group', 'in', 'a', 'suburb', 'of', 'damascus', 'while', 'losing', 'her', 'son', 'george', 'after', 'he', 'refused', 'to', 'identify', 'himself', 'to', 'the', 'terrorists', 'by', 'a', 'muslim', 'name', 'the', 'woman', 'alice', 'assaf', 'said', 'that', 'she', 'had', 'heard', 'that', 'her', 'son', 'who', 'was', 'beaten', 'and', 'shot', 'to', 'death', 'was', 'spared', 'a', 'much', 'worse', 'fate', 'being', 'baked', 'in', 'the', 'ovens', 'of', 'the', 'local', 'bakery', 'or', 'kneaded', 'to', 'death', 'in', 'the', 'bakery', 's', 'industrial', 'sized', 'mixer', 'assaf', 'told', 'her', 'story', 'to', 'roads', 'of', 'success', 'a', 'human', 'rights', 'ngo', 'with', 'a', 'focus', 'on', 'the', 'rights', 'of', 'women', 'and', 'christians', 'in', 'the', 'middle', 'east', 'the', 'group', 'has', 'provided', 'testimony', 'of', 'the', 'plight', 'of', 'christians', 'and', 'yazidis', 'persecuted', 'by', 'the', 'islamic', 'state', 'to', 'the', 'house', 'foreign', 'relations', 'committee', 'and', 'regularly', 'publishes', 'video', 'interviews', 'with', 'isis', 'attack', 'survivors', 'the', 'christian', 'post', 'picked', 'up', 'the', 'story', 'and', 'video', 'interview', 'in', 'which', 'assaf', 'narrates', 'the', 'final', 'days', 'of', 'her', '18', 'year', 'old', 's', 'life', 'this', 'article', 'was', 'posted', 'wednesday', 'october', '26', '2016', 'at', '6', '30', 'am', 'share', 'this', 'article']\n"
     ]
    }
   ],
   "source": [
    "print(new_content_token_csv) #some of the news_content_token_csv is float type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "news_content_all_csv= []\n",
    "for i in range (0,len(fake_news_content_from_csv_str)):\n",
    "    new_content_token_csv = re.sub(r'[^\\w]', ' ', fake_news_content_from_csv_str[i]).lower().split()\n",
    "    new_content_token_csv = [ word for word in new_content_token_csv if word not in stopwords.words('english')]\n",
    "    news_content_all_csv.append(new_content_token_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1507\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(fake_news_content_from_csv))\n",
    "print(len(fake_news_content_from_csv))\n",
    "print(type(fake_news_content_from_csv[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract news content from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['print', 'pay', 'back', 'money', 'plus', 'interest', 'entire', 'family', 'everyone', 'came', 'need', 'deported', 'asap', 'take', 'two', 'years', 'bust', 'go', 'another', 'group', 'stealing', 'government', 'taxpayers', 'group', 'somalis', 'stole', 'four', 'million', 'government', 'benefits', '10', 'months', 'reported', 'numerous', 'cases', 'like', 'one', 'muslim', 'refugees', 'immigrants', 'commit', 'fraud', 'scamming', 'system', 'way', 'control', 'related']\n",
      "['josh', 'stop', 'talking', 'sit', 'american', 'people', 'fed', 'spewing', 'lies', 'coming', 'administration', 'destruction', 'country', 'come', 'end', 'people', 'spoken', 'time', 'change', 'long', 'time', 'coming', 'trump', 'would', 'say', 'time', 'drain', 'swamp']\n"
     ]
    }
   ],
   "source": [
    "print(news_content_all_csv[0])\n",
    "print(news_content_all_csv[1333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to a list of string\n",
    "news_content_string_csv = [' '.join(x) for x in news_content_all_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['print pay back money plus interest entire family everyone came need deported asap take two years bust go another group stealing government taxpayers group somalis stole four million government benefits 10 months reported numerous cases like one muslim refugees immigrants commit fraud scamming system way control related', 'attorney general loretta lynch plead fifth barracuda brigade 2016 10 28 print administration blocking congressional probe cash payments iran course needs plead 5th either recall refuses answer plain deflects question straight corruption finest 100percentfedup com talk covering ass loretta lynch plead fifth avoid incriminating payments iran corrupt core attorney general loretta lynch declining comply investigation leading members congress obama administration secret efforts send iran 1 7 billion cash earlier year prompting accusations lynch pleaded fifth amendment avoid incriminating payments according lawmakers communications exclusively obtained washington free beacon sen marco rubio r fla rep mike pompeo r kan initially presented lynch october series questions cash payment iran approved delivered oct 24 response assistant attorney general peter kadzik responded lynch behalf refusing answer questions informing lawmakers barred publicly disclosing details cash payment bound ransom deal aimed freeing several american hostages iran response attorney general office unacceptable provides evidence lynch chosen essentially plead fifth refuse respond inquiries regarding role providing cash world foremost state sponsor terrorism rubio pompeo wrote friday follow letter lynch related']\n"
     ]
    }
   ],
   "source": [
    "print(news_content_string_csv[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label the news from csv files, all news are fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# create a list to hold the response variable fake and real (1 and 0), the first 91 is fake and last 91 is real\n",
    "fake_y_csv = [1] * len(news_content_string_csv)\n",
    "\n",
    "# one hot encode the output variable\n",
    "news_Y_csv = np.array (fake_y_csv)\n",
    "print(type(news_Y_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1461,)\n"
     ]
    }
   ],
   "source": [
    "print(news_Y_csv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read all json files from different folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read real\n",
    "myList = []\n",
    "directory = '/Users/moranwang/Documents/MSofDS/SummerQuarterDS/Capstone/FakeNewsNet/Data/BuzzFeed/FakeNewsContent'\n",
    "file_names = os.listdir(directory)\n",
    "cleaned_file_name = [f for f in file_names if f.endswith('.json')]\n",
    "#for i, filename in zip(range(1),cleaned_file_name):\n",
    "for filename in cleaned_file_name:   \n",
    "    with open(os.path.join(directory,filename),'r',encoding=\"utf8\") as myfile:\n",
    "        #print(filename)\n",
    "        data = json.load(myfile) \n",
    "        myList.append(data)\n",
    "        \n",
    "# read fake\n",
    "myList_real = []\n",
    "my_df_real  = pd.DataFrame()\n",
    "directory_real = '/Users/moranwang/Documents/MSofDS/SummerQuarterDS/Capstone/FakeNewsNet/Data/BuzzFeed/RealNewsContent'\n",
    "file_names_real = os.listdir(directory_real)\n",
    "cleaned_file_name_real = [f for f in file_names_real if f.endswith('.json')]\n",
    "#for i, filename in zip(range(1),cleaned_file_name):\n",
    "for filename in cleaned_file_name_real:   \n",
    "    with open(os.path.join(directory_real,filename),'r',encoding=\"utf8\") as myfile:\n",
    "        #print(filename)\n",
    "        data = json.load(myfile) \n",
    "        myList_real.append(data)\n",
    "        \n",
    "# read fake news\n",
    "myList_fake = []\n",
    "my_df  = pd.DataFrame()\n",
    "directory_fake = '/Users/moranwang/Documents/MSofDS/SummerQuarterDS/Capstone/FakeNewsNet/Data/PolitiFact/FakeNewsContent'\n",
    "file_names_fake = os.listdir(directory_fake)\n",
    "cleaned_file_name_fake = [f for f in file_names_fake if f.endswith('.json')]\n",
    "#for i, filename in zip(range(1),cleaned_file_name):\n",
    "for filename in cleaned_file_name_fake:   \n",
    "    with open(os.path.join(directory_fake,filename),'r',encoding=\"utf8\") as myfile:\n",
    "        #print(filename)\n",
    "        data = json.load(myfile) \n",
    "        myList_fake.append(data)\n",
    "        \n",
    "# read good news\n",
    "myList_real_p = []\n",
    "directory_real_p = '/Users/moranwang/Documents/MSofDS/SummerQuarterDS/Capstone/FakeNewsNet/Data/PolitiFact/RealNewsContent'\n",
    "file_names_real_p = os.listdir(directory_real_p)\n",
    "cleaned_file_name_real_p = [f for f in file_names_real_p if f.endswith('.json')]\n",
    "#for i, filename in zip(range(1),cleaned_file_name):\n",
    "for filename in cleaned_file_name_real_p:   \n",
    "    with open(os.path.join(directory_real_p,filename),'r',encoding=\"utf8\") as myfile:\n",
    "        #print(filename)\n",
    "        data = json.load(myfile) \n",
    "        myList_real_p.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append all file together (fake and real) from both json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = myList + myList_fake + myList_real + myList_real_p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    }
   ],
   "source": [
    "print(len(news_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract news content from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# news_content_all has already delete the stopwards.words\n",
    "import re \n",
    "news_content_all = []\n",
    "for i in range (0,len(news_list)): # i from 0 to 90\n",
    "    #print(news_dict)\n",
    "    per_new_text= []\n",
    "    news_dict = news_list[i]\n",
    "    news_content = news_dict['text']\n",
    "    new_content_token = re.sub(r'[^\\w]', ' ', news_content).lower().split()\n",
    "    new_content_token = [ word for word in new_content_token if word not in stopwords.words('english')]\n",
    "    news_content_all.append(new_content_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n",
      "232\n",
      "160\n",
      "the type of news_content_all is  <class 'list'>\n",
      "the type fo news content all [0] is <class 'list'>\n",
      "people\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(news_content_all))\n",
    "print(len(news_content_all[0]))\n",
    "print(len(news_content_all[90]))\n",
    "print(\"the type of news_content_all is \", type(news_content_all))\n",
    "print(\"the type fo news content all [0] is\", type(news_content_all[0]))\n",
    "print(news_content_all[0][0])\n",
    "# this news_content_all is a list of list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of list to a list of string, so we can use in the Tokenizer part\n",
    "news_content_string = [' '.join(x) for x in news_content_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "people noticed something odd hillary outfit debate last night lot could discussed last night debate like unfair debate moderator asking questions help hillary main concern right one thing baffled many hillary appear usually stupid coughing self ready answers detailed facts almost bizarre sputter reach glass water odd everyone grandma talking latest question swirling around social media pictures appeared show hillary kind flesh colored device embedded inside ear earpiece wearing something fashionable old ladies twitter may figured answer see certainly first time hillary use device help think could trick light shadows puffy zipper perhaps hillary actually hiding cough prevention machine hideous feminist pantsuit hillary clinton campaign fervently denying wearing earpiece 101 things young adults know sir john hawkins john hawkins book 101 things young adults know filled lessons newly minted adults need order get life gleaned lifetime trial error writing hawkins provides advice everyone benefit short digestible chapters buy fox news reported clinton spokesman nick merrill said seen photo merely reflection tv lights flash sure going inside hillary little head last night interesting email posted woods clinton huma abedin wikileaks archive released u take earpiece need get woods hidden sense humor escaped twitter even lie without help gaggle liars earpiece woods added literally help kind sociopath lie even truth beneficial course actual proof earpiece could hearing aid hmmm one talked hillary wearing earpiece past fumbled speeches claim merely reflection studio lights let judge\n"
     ]
    }
   ],
   "source": [
    "print(type(news_content_string))\n",
    "print(news_content_string[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to create news_content all, a list of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# news_content_all doesn't delete the stopwards words\\nimport re \\nnews_content_all = []\\nfor i in range (0,len(news_list)): # i from 0 to 90\\n    per_new_text= []\\n    news_dict = news_list[i]\\n    news_content = news_dict['text']\\n    new_content_token = re.sub(r'[^\\\\w]', ' ', news_content).lower()\\n    news_content_all.append(new_content_token)\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# news_content_all doesn't delete the stopwards words\n",
    "import re \n",
    "news_content_all = []\n",
    "for i in range (0,len(news_list)): # i from 0 to 90\n",
    "    per_new_text= []\n",
    "    news_dict = news_list[i]\n",
    "    news_content = news_dict['text']\n",
    "    new_content_token = re.sub(r'[^\\w]', ' ', news_content).lower()\n",
    "    news_content_all.append(new_content_token)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(news_content_all[0])\\nprint(type(news_content_all[0]))\\nprint(type(news_content_all))\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(news_content_all[0])\n",
    "print(type(news_content_all[0]))\n",
    "print(type(news_content_all))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count the word frequency in the entire news and save all words we want to keep , delete words that we think is not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_content_string is a list of string, that are used for embedding \n",
    "# news_content_all is a list of list \n",
    "# make nested list to flat list and then count frequency \n",
    "from collections import Counter\n",
    "import itertools\n",
    "flatList = list(itertools.chain.from_iterable(news_content_all))\n",
    "counts = Counter(flatList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n",
      "15605\n"
     ]
    }
   ],
   "source": [
    "print(type(counts))\n",
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n3798 word frequent is greater than 5. \\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for key, cnts in list(counts.items()):   \n",
    "    if cnts > 5:\n",
    "        count = count + 1\n",
    "print(count) \n",
    "\n",
    "'''\n",
    "3798 word frequent is greater than 5. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create response variable y fake is 1 and real is 0, y is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(1883,)\n"
     ]
    }
   ],
   "source": [
    "# create a list to hold the response variable fake and real (1 and 0), the first 91 is fake and last 91 is real\n",
    "fake_y = [1] * (91 + 120)\n",
    "real_y = [0]* (91+120)\n",
    "fake_real_label = []\n",
    "fake_real_label.extend(fake_y)\n",
    "fake_real_label.extend(real_y)\n",
    "print(len(fake_real_label))\n",
    "print(type(fake_real_label))\n",
    "\n",
    "# one hot encode the output variable\n",
    "news_Y = np.array (fake_real_label)\n",
    "news_Y = np.concatenate([news_Y_csv,news_Y])\n",
    "#news_Y.extend(news_Y_csv)\n",
    "print(type(news_Y))\n",
    "print(news_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add news_content_string and news_content_string_csv together, label should be all fake and real\n",
    "docs = news_content_string_csv + news_content_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print pay back money plus interest entire family everyone came need deported asap take two years bust go another group stealing government taxpayers group somalis stole four million government benefits 10 months reported numerous cases like one muslim refugees immigrants commit fraud scamming system way control related'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of news: 1883\n",
      "type of news: <class 'list'>\n",
      "length of label: 1883\n",
      "type of news: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"length of news:\", len(docs))\n",
    "print(\"type of news:\", type(docs))\n",
    "\n",
    "print(\"length of label:\", len(news_Y))\n",
    "print(\"type of news:\", type(news_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import package about keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the tokenizer Part and padding words to integer and then to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 把词转换为编号，词的编号根据词频设定，频率越大，编号越小\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_embbeding  = t.texts_to_sequences(docs)\n",
    "\n",
    "'''\n",
    "# 把词转换为编号，词的编号根据词频设定，频率越大，编号越小\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "47\n",
      "<class 'list'>\n",
      "[535, 197, 2573, 871, 8571, 3484, 13791, 6857, 18, 101, 665, 946, 255, 3707, 1044, 2404, 1295, 2694, 348, 261, 415, 8571, 9428, 334, 3942, 3367, 657, 3277, 10502, 251, 1296, 827, 10503, 11875, 22, 436, 3032, 2057, 2573, 871, 8571, 3484, 920, 7891, 2694, 348, 993, 2167, 535, 197, 2573, 871, 6079, 5472, 153, 745, 393, 424, 20, 255, 728, 631, 1269, 348, 88, 361, 746, 1295, 401, 62, 4582, 2217, 871, 5241, 3484, 1849, 920, 7891, 2694, 72, 2266, 1906, 3943, 2624, 129, 131, 6080, 1143, 3821, 2058, 1045, 7892, 1238, 1297, 10504, 1045, 16551, 2948, 2819, 871, 73, 1057, 605, 1295, 3708, 348, 1672, 2104, 570, 888, 689, 2218, 535, 197, 1628, 1205, 1553, 871, 2267, 3033, 657, 605, 6081, 2266, 5756, 1629, 10505, 1415, 1295, 3708, 4402, 7334, 438, 2508, 11876, 233, 34, 8572, 348, 689, 535, 197, 172, 5473, 1320, 280, 871, 2105, 2018, 8571, 3484, 2949, 2268, 6082, 1079, 550, 1907, 1295, 33, 6858, 19, 8573, 439, 2058, 10504, 304, 658, 695, 499, 871, 403]\n"
     ]
    }
   ],
   "source": [
    "print(type(news_embbeding))\n",
    "print(len(news_embbeding[0]))\n",
    "print(type(news_embbeding[0]))\n",
    "print(news_embbeding[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 426\n",
    "padded_docs = pad_sequences(news_embbeding, maxlen=max_length) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,    36,   211,   403],\n",
       "       [    0,     0,     0, ...,   499,   871,   403],\n",
       "       [    0,     0,     0, ...,  8575,  1058,   403],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  3268,     2,   843],\n",
       "       [    0,     0,     0, ...,  5586,  1878,  1501],\n",
       "       [33878,  1573,  3594, ...,  1180,  2446,  1434]], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npad_sequences is a function that can make different length of news become same length, if the length of a news exceed \\nthe maxlen, then this function will make the words that exceed max length to 0 and if the length of a news is less than\\nthe maxlen, then this function will make the different length of words between current length and max length to 0. \\n\\nSo, how to set the max length? how to determine what element should be left or delete? \\n\\nbefore doing pad_sequences, we decide to count frequency of words in all news then rank them. Save the words with \\nhighest and useful words. (except I , am , and, my)\\n\\nSaving the words that we want to keep to a new trainX and redo the new embedding. \\n\\n\\nBy the way, the embedding part is translate each word to a unqiue integer. \\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pad_sequences is a function that can make different length of news become same length, if the length of a news exceed \n",
    "the maxlen, then this function will make the words that exceed max length to 0 and if the length of a news is less than\n",
    "the maxlen, then this function will make the different length of words between current length and max length to 0. \n",
    "\n",
    "So, how to set the max length? how to determine what element should be left or delete? \n",
    "\n",
    "before doing pad_sequences, we decide to count frequency of words in all news then rank them. Save the words with \n",
    "highest and useful words. (except I , am , and, my)\n",
    "\n",
    "Saving the words that we want to keep to a new trainX and redo the new embedding. \n",
    "\n",
    "\n",
    "By the way, the embedding part is translate each word to a unqiue integer. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795\n"
     ]
    }
   ],
   "source": [
    "# check how many words appears more than 5 times, \n",
    "count = 0\n",
    "for key, cnts in list(counts.items()):   \n",
    "    if cnts > 30:\n",
    "        count = count + 1\n",
    "print(count) \n",
    "\n",
    "# greater than 20, 1213 words\n",
    "# greater than 30, 795 words\n",
    "# greater than 50, 426 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncannot sort it, because after sort the order of words will change \\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cannot sort it, because after sort the order of words will change \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 35, 36, 38, 49, 50, 50, 54, 74, 83, 94, 98, 101, 126, 128, 141, 141, 179, 185, 211, 220, 244, 282, 317, 354, 392, 403, 569, 616, 810, 870, 912, 933, 946, 992, 1394, 1601, 2572, 3594, 4236, 4401, 7890, 9427, 16549, 16550, 21669]\n"
     ]
    }
   ],
   "source": [
    "# pad_sequences process based on rank\n",
    "# Example to sort the first element\n",
    "e1 = news_embbeding[0]\n",
    "print(sorted(e1,key = int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(x) for x in news_embbeding]\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.511e+03, 2.790e+02, 6.500e+01, 1.500e+01, 5.000e+00, 2.000e+00,\n",
       "        1.000e+00, 4.000e+00, 0.000e+00, 1.000e+00]),\n",
       " array([   0. ,  417.2,  834.4, 1251.6, 1668.8, 2086. , 2503.2, 2920.4,\n",
       "        3337.6, 3754.8, 4172. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a6f5208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987\n"
     ]
    }
   ],
   "source": [
    "length = sorted(length)\n",
    "length\n",
    "\n",
    "index = int(len(length)* 0.97) # 1171 cover 97% of words\n",
    "max_length = length[index]\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "padded_docs = pad_sequences(news_embbeding, maxlen=max_length, padding = 'post') \n",
    "\n",
    "# choose save the words which frequent greater than 50?\n",
    "# padding: String, 'pre' or 'post': pad either before or after each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 946  392   49 ...    0    0    0]\n",
      " [ 535  197 2573 ...    0    0    0]\n",
      " [ 576   19  464 ...    0    0    0]\n",
      " ...\n",
      " [ 653 4087 1110 ...    0    0    0]\n",
      " [3827    3 1977 ...    0    0    0]\n",
      " [  66    5 1630 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_docs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1883"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use glove to do word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "embeddings_index = dict()\n",
    "file = \"/Users/moranwang/Documents/MSofDS/SummerQuarterDS/glove/glove.6B.50d.txt\"\n",
    "f = open(file)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Next, we need to create a matrix of one embedding for each word in the training dataset. \n",
    "We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight \n",
    "vector from the loaded GloVe embedding.\n",
    "The result is a matrix of weights only for words we will see during training.\n",
    "'''\n",
    "vocab_size = len(t.word_index) + 1\n",
    "from numpy import zeros\n",
    "embedding_matrix = zeros((vocab_size, 50))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33893\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(len(t.word_index)) #  how many words in the entire words\n",
    "print(type(t.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.45769     0.85191     0.31097999 ... -0.74225998  0.77052999\n",
      "   0.26394001]\n",
      " [ 0.23158     0.69963998  0.43878001 ...  0.68463999 -0.43252999\n",
      "   0.75606   ]\n",
      " ...\n",
      " [ 0.43803999 -0.28209999  0.76776999 ...  1.10179996  0.51640999\n",
      "   0.37685999]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.0038993  -0.37799999  0.22160999 ...  0.79258001  0.72994\n",
      "   0.21297   ]]\n",
      "33894\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(t.word_index)\n",
    "\n",
    "value = [v for key,v in t.word_index.items()]\n",
    "value.count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using training data to train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split training , validation and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1883\n",
      "1883\n"
     ]
    }
   ],
   "source": [
    "print(len(padded_docs))\n",
    "print(len(news_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "validation_size = 0.2\n",
    "seed = 3\n",
    "# write your code to complete following line\n",
    "\n",
    "train_X, test_X = train_test_split(padded_docs,  test_size=validation_size, random_state=seed)\n",
    "train_y, test_y = train_test_split(news_Y,  test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training x is 1506\n",
      "the training y is 1506\n",
      "the testing x is 377\n",
      "the testing y is 377\n",
      "[1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[  207    17    18 ...     0     0     0]\n",
      " [  114    97   821 ...     0     0     0]\n",
      " [   11    44    28 ...     0     0     0]\n",
      " ...\n",
      " [  383  6740   383 ...     0     0     0]\n",
      " [  106  2305  2073 ...     0     0     0]\n",
      " [ 2348 10483  2320 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "#print(len(train_X[0]))\n",
    "#print(len(train_X[1]))\n",
    "print('the training x is', len(train_X))\n",
    "print('the training y is', len(train_y))\n",
    "print('the testing x is', len(test_X))\n",
    "print('the testing y is', len(test_y))\n",
    "\n",
    "\n",
    "print(train_y[0:20])\n",
    "print(test_X[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solve data imbalance problem -- Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"Number transactions X_train dataset: \", train_X.shape)\n",
    "print(\"Number transactions y_train dataset: \", train_y.shape)\n",
    "print(\"Number transactions X_test dataset: \", test_X.shape)\n",
    "print(\"Number transactions y_test dataset: \", test_y.shape)\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(train_y==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(train_y==0)))\n",
    "\n",
    "# oversample on training set\n",
    "sm = SMOTE(random_state=12)\n",
    "x_train_res, y_train_res = sm.fit_sample(train_X, train_y)\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use tensorflow to handle class imbalance? (88% of data are fake) -- the mothod below looks like dosen't works on LSTM\n",
    "## add class weights to the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# function to weighted the loss of predicting the 0. deal with the data imbalance\\nfrom keras import backend as K\\nfrom __future__ import print_function\\nimport numpy as np\\n#np.random.seed(1337)  # for reproducibility\\n\\nfrom keras.datasets import mnist\\nfrom keras.models import Sequential\\nfrom keras.layers.core import Dense, Dropout, Activation\\nfrom keras.optimizers import SGD, Adam, RMSprop\\nfrom keras.utils import np_utils\\nimport keras.backend as K\\nfrom itertools import product\\n\\n\\nfrom keras import backend as K\\ndef weighted_categorical_crossentropy(weights):\\n    \"\"\"\\n    A weighted version of keras.objectives.categorical_crossentropy\\n    \\n    Variables:\\n        weights: numpy array of shape (C,) where C is the number of classes\\n    \\n    Usage:\\n        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\\n        loss = weighted_categorical_crossentropy(weights)\\n        model.compile(loss=loss,optimizer=\\'adam\\')\\n    \"\"\"\\n    \\n    weights = K.variable(weights)\\n        \\n    def loss(y_true, y_pred):\\n        # scale predictions so that the class probas of each sample sum to 1\\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\\n        # clip to prevent NaN\\'s and Inf\\'s\\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\\n        # calc\\n        loss = y_true * K.log(y_pred) * weights\\n        loss = -K.sum(loss, -1)\\n        return loss\\n    \\n    return loss\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# function to weighted the loss of predicting the 0. deal with the data imbalance\n",
    "from keras import backend as K\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "import keras.backend as K\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# add embedding layers\n",
    "model.add(Embedding(vocab_size, 50,weights=[embedding_matrix], input_length=max_length)) # length of dictionary, dim output\n",
    "\n",
    "layer = model.layers[0]\n",
    "layer.trainable = False\n",
    "\n",
    "batchSize = 128\n",
    "#batchSize = 1  # should be 128\n",
    "weights = np.array([0.5,8])\n",
    "\n",
    "\n",
    "model.add(LSTM(40, input_shape =(None,50)))  #should be 40\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.compile(loss=weighted_categorical_crossentropy(weights), optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit model and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(x_train_res, y_train_res, batch_size=batchSize, validation_data= (test_X, test_y), epochs=50)\n",
    "model.fit(train_X, train_y, batch_size=batchSize, validation_data= (test_X, test_y), epochs=50)\n",
    "model.save('finalModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# commen the next cell's code (in order to do API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to do the next part, just select 100 news form x_train and y_train, and 50 news\n",
    "# from test_x and test_y\n",
    "#x_train_100 = train_X[0:50]\n",
    "#y_train_100 = train_y[0:50]\n",
    "#test_X_100 = test_X [0:10]\n",
    "#test_y_100 = test_y [0:10]\n",
    "\n",
    "#model.fit(x_train_100, y_train_100, batch_size=batchSize, validation_data= (test_X_100, test_y_100), epochs=1)\n",
    "\n",
    "#model.save('finalModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read the url and make it to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from keras.applications import ResNet50\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications import imagenet_utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import flask\n",
    "import io\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-f740b53d6913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#soup.prettify()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"story-body__inner\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0marticle_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "url = 'https://www.bbc.com/news/world-middle-east-45128367'\n",
    "\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "table = soup.find(\"div\", {\"class\":\"story-body__inner\"}).findAll('p')\n",
    "\n",
    "\n",
    "article_text = ''\n",
    "for element in table:\n",
    "    article_text += ''.join(element.findAll(text = True))\n",
    "\n",
    "article_text = re.sub(r'[^\\w]', ' ', article_text)\n",
    "article_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head><title>403 Forbidden</title></head>\n",
       "<body bgcolor=\"white\">\n",
       "<center><h1>403 Forbidden</h1></center>\n",
       "<hr/><center>nginx</center>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the tokenizer on the documents\n",
    "#t.fit_on_texts(article_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_news_embbeding = t.texts_to_sequences(article_text)\n",
    "\n",
    "padded_docs_p = pad_sequences(text_news_embbeding, maxlen=max_length) \n",
    "    \n",
    "padded_docs_p = pad_sequences(text_news_embbeding, maxlen=987, padding = 'post') \n",
    "    \n",
    "#vocab_size = len(t.word_index) + 1\n",
    "#from numpy import zeros\n",
    "#embedding_matrix_p = zeros((vocab_size, 50))\n",
    "#for word, i in t.word_index.items():\n",
    "#    embedding_vector_p = embeddings_index.get(word)\n",
    "#    if embedding_vector_p is not None:\n",
    "#        embedding_matrix_p[i] = embedding_vector_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1506, 987)\n",
      "[[ 3  0  0 ...  0  0  0]\n",
      " [ 2  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 2  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [26  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(padded_docs_p[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_2_input to have shape (987,) but got array with shape (50,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-43c3088e34b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             warnings.warn('Network returning invalid probability values. '\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                              'argument.')\n\u001b[1;32m   1151\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    134\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_2_input to have shape (987,) but got array with shape (50,)"
     ]
    }
   ],
   "source": [
    "model = load_model('finalModel')\n",
    "#model.add(Embedding(vocab_size, 50,weights=[embedding_matrix_p], input_length=max_length)) \n",
    "\n",
    "preds = model.predict(padded_docs_p) # input is the sequence of the text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is the actual code will be used in this project  --- train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2680 samples, validate on 377 samples\n",
      "Epoch 1/1\n",
      " 118/2680 [>.............................] - ETA: 25:34 - loss: 0.6898 - acc: 0.5678"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-e6f66a1c39b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#model.fit(train_X, train_y, batch_size=batchSize, validation_data= (test_X, test_y), epochs=50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model.fit(train_X, train_y, batch_size=batchSize, validation_data= (test_X, test_y), epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "#model.fit(train_X, train_y, batch_size=batchSize, validation_data= (test_X, test_y), epochs=50)\n",
    "#model.fit(train_X, train_y, batch_size=batchSize, validation_data= (test_X, test_y), epochs=1)\n",
    "######### running this line of code \n",
    "#model.fit(x_train_res, y_train_res, batch_size=batchSize, validation_data= (test_X, test_y), epochs=1)\n",
    "\n",
    "# evaluate the model\n",
    "#loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
